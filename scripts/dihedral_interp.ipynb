{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IMIMIXuoqUT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import os, sys\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install packages\n",
        "    %pip install einops\n",
        "    %pip install jaxtyping\n",
        "    %pip install transformer_lens\n",
        "    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    import os, sys\n",
        "    if not os.path.exists(\"chapter1_transformers\"):\n",
        "        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n",
        "        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n",
        "        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n",
        "        os.rmdir(\"ARENA_2.0-main\")\n",
        "        os.chdir(\"chapter1_transformers/exercises\")\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "    CHAPTER = r\"chapter1_transformers\"\n",
        "    CHAPTER_DIR = r\"./\" if CHAPTER in os.listdir() else os.getcwd().split(CHAPTER)[0]\n",
        "    EXERCISES_DIR = CHAPTER_DIR + f\"{CHAPTER}/exercises\"\n",
        "    sys.path.append(EXERCISES_DIR)\n",
        "\n",
        "%pip install s3fs\n",
        "%pip install omegaconf\n",
        "%pip install git+https://github.com/CindyXWu/devinterp-automata.git\n",
        "%pip install torch-ema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "87aqhevcowv4",
        "outputId": "447bcefc-d249-4f0a-9d2c-464e5a6ac970"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import plotly.express as px\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import einops\n",
        "from jaxtyping import Int, Float\n",
        "from typing import List, Optional, Tuple\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import webbrowser\n",
        "import gdown\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "import circuitsvis as cv\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAIN = __name__ == \"__main__\"\n",
        "\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import os\n",
        "import yaml\n",
        "import s3fs\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from di_automata.config_setup import *\n",
        "from di_automata.constructors import (\n",
        "    construct_model,\n",
        "    create_dataloader_hf,\n",
        ")\n",
        "from di_automata.tasks.data_utils import take_n\n",
        "\n",
        "# AWS\n",
        "s3 = s3fs.S3FileSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "UBiOhwUcsAPm",
        "outputId": "f3e67704-4c40-4904-dd72-3d81e81eb688"
      },
      "outputs": [],
      "source": [
        "git clone https://github.com/CindyXWu/devinterp-automata.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "yNLip8nbp6vZ",
        "outputId": "efd344b0-c527-46dd-dd4d-879061d0e69d"
      },
      "outputs": [],
      "source": [
        "config_file_path = \"configs/slt_config.yaml\"\n",
        "slt_config = OmegaConf.load(config_file_path)\n",
        "\n",
        "with open(f\"configs/task_config/{slt_config.dataset_type}.yaml\", 'r') as file:\n",
        "    task_config = yaml.safe_load(file)\n",
        "\n",
        "OmegaConf.set_struct(slt_config, False) # Allow new configuration values to be added\n",
        "slt_config[\"task_config\"] = task_config\n",
        "# Convert OmegaConf object to MainConfig Pydantic model for dynamic type validation - NECESSARY DO NOT SKIP\n",
        "pydantic_config = PostRunSLTConfig(**slt_config)\n",
        "# Convert back to OmegaConf object for compatibility with existing code\n",
        "slt_config = OmegaConf.create(pydantic_config.model_dump())\n",
        "\n",
        "print(task_config[\"dataset_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "PEFYiG-EqAuR",
        "outputId": "7d220dd5-6911-4ba8-fa4e-33e941701499"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Run path and name for easy referral later\n",
        "run_path = f\"{slt_config.entity_name}/{slt_config.wandb_project_name}\"\n",
        "run_name = slt_config.run_name\n",
        "\n",
        "# Get run information\n",
        "api = wandb.Api(timeout=3000)\n",
        "run_list = api.runs(\n",
        "    path=run_path,\n",
        "    filters={\n",
        "        \"display_name\": run_name,\n",
        "        \"state\": \"finished\",\n",
        "        },\n",
        "    order=\"created_at\", # Default descending order so backwards in time\n",
        ")\n",
        "assert run_list, f\"Specified run {run_name} does not exist\"\n",
        "run_api = run_list[slt_config.run_idx]\n",
        "try: history = run_api.history()\n",
        "except: history = run_api.history\n",
        "loss_history = history[\"Train Loss\"]\n",
        "accuracy_history = history[\"Train Acc\"]\n",
        "steps = history[\"_step\"]\n",
        "time = run_api.config[\"time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYkD7_cIqEIh"
      },
      "outputs": [],
      "source": [
        "def get_config() -> MainConfig:\n",
        "    \"\"\"\"\n",
        "    Manually get config from run as artifact.\n",
        "    WandB also logs automatically for each run, but it doesn't log enums correctly.\n",
        "    \"\"\"\n",
        "    artifact = api.artifact(f\"{run_path}/config:{run_name}_{time}\")\n",
        "    data_dir = artifact.download()\n",
        "    config_path = Path(data_dir) / \"config.yaml\"\n",
        "    return OmegaConf.load(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgFUGT2lqHoA"
      },
      "outputs": [],
      "source": [
        "config = get_config()\n",
        "\n",
        "# Set total number of unique samples seen (n). If this is not done it will break LLC estimator.\n",
        "slt_config.rlct_config.sgld_kwargs.num_samples = slt_config.rlct_config.num_samples = config.rlct_config.sgld_kwargs.num_samples\n",
        "slt_config.nano_gpt_config = config.nano_gpt_config\n",
        "\n",
        "model, param_inf_properties = construct_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utSRu7dAqPbr"
      },
      "outputs": [],
      "source": [
        "def restore_state_single_cp(cp_idx: int) -> dict:\n",
        "    \"\"\"Restore model state from a single checkpoint.\n",
        "    Used in _load_logits_states() and _calculate_rlct().\n",
        "\n",
        "    Args:\n",
        "        idx_cp: index of checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        model state dictionary.\n",
        "    \"\"\"\n",
        "    idx = cp_idx * config.rlct_config.ed_config.eval_frequency\n",
        "    print(config.model_save_method)\n",
        "    match config.model_save_method:\n",
        "        case \"wandb\":\n",
        "            artifact = api.artifact(f\"{run_path}/states:idx{idx}_{run_name}_{time}\")\n",
        "            data_dir = artifact.download()\n",
        "            state_path = Path(data_dir) / f\"states_{idx}.torch\"\n",
        "            states = torch.load(state_path)\n",
        "        case \"aws\":\n",
        "            with s3.open(f'{config.aws_bucket}/{run_name}_{time}/states_{idx}.pth', mode='rb') as file:\n",
        "                states = torch.load(file)\n",
        "    return states[\"model\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYVaCV7BqiBy"
      },
      "outputs": [],
      "source": [
        "current_directory = Path().absolute()\n",
        "logits_file_path = current_directory.parent / f\"di_automata/logits_{run_name}_{time}\"\n",
        "print(logits_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIMW7veTqkPu"
      },
      "outputs": [],
      "source": [
        "cusp_idx = 620\n",
        "cp_idx = cusp_idx // config.rlct_config.ed_config.eval_frequency\n",
        "print(cp_idx)\n",
        "print(time)\n",
        "\n",
        "state = restore_state_single_cp(cp_idx)\n",
        "model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7ZyZvIAqmq6"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMSwFPbSqnZE"
      },
      "outputs": [],
      "source": [
        "ed_loader = create_dataloader_hf(config, deterministic=True) # Make sure deterministic to see same data\n",
        "for data in take_n(ed_loader, 1):\n",
        "    inputs = data[\"input_ids\"]\n",
        "    logits, cache = model.run_with_cache(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPif_gxLqrH0"
      },
      "outputs": [],
      "source": [
        "att_1 = cache[\"pattern\", 0, \"attn\"]\n",
        "print(att_1)\n",
        "display(cv.attention.attention_patterns(\n",
        "    tokens=inputs,\n",
        "    attention=att_1,\n",
        "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
