{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description of this notebook\n",
        "- This notebook assumes access to my WandB API key and Dashiell's AWS bucket.\n",
        "- This is a 5-state (10 total), 3-layer, 4-heads-per-layer HookedTransformer transformer with layernorm and MLPs.\n",
        "- WandB run this pertains to (private): https://wandb.ai/wu-cindyx/devinterp-automata/runs/89pbzt28?nw=nwuserwucindyx.\n",
        "\n",
        "This contains:\n",
        "- Activation distribution PCA code and plots (here we just tried resid_mid and mlp_post, but all others are testable)\n",
        "- Attention patterns for all heads in all layers on random inputs\n",
        "    - These are done for all forms and also at end of the training process: we have 4 forms and a final model. model_0 is a comparison for very early on in training.\n",
        "- Attention patterns for all heads in all layers on hand-crafted examples for sequences of all 0s, all 1s, all 0s with a 1 injected at a single position, and the inverse of the latter (all 1s with a 0 injected at a single position)\n",
        "- Positional embedding and embedding patching, including dot product of columns of W_pos with itself\n",
        "- OV circuit analysis, including eigenvectors to search for copying circuits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IMIMIXuoqUT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import os, sys\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    if not os.path.exists(\"chapter1_transformers\"):\n",
        "        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n",
        "        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n",
        "        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n",
        "        os.rmdir(\"ARENA_2.0-main\")\n",
        "\n",
        "         # Install packages\n",
        "        %pip install einops\n",
        "        %pip install jaxtyping\n",
        "        %pip install transformer_lens\n",
        "        %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "        %pip install s3fs\n",
        "        %pip install omegaconf\n",
        "        %pip install git+https://github.com/CindyXWu/devinterp-automata.git\n",
        "        %pip install torch-ema\n",
        "\n",
        "        !curl -o /content/main.zip https://codeload.github.com/CindyXWu/devinterp-automata/zip/refs/heads/main\n",
        "        !unzip -o /content/main.zip -d /content/\n",
        "\n",
        "        sys.path.append(\"/content/devinterp-automata/\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "\n",
        "        os.chdir(\"chapter1_transformers/exercises\")\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "    CHAPTER = r\"chapter1_transformers\"\n",
        "    CHAPTER_DIR = r\"./\" if CHAPTER in os.listdir() else os.getcwd().split(CHAPTER)[0]\n",
        "    EXERCISES_DIR = CHAPTER_DIR + f\"{CHAPTER}/exercises\"\n",
        "    sys.path.append(EXERCISES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87aqhevcowv4",
        "outputId": "0e2bd4b2-192c-48c6-939d-8ed81aa8ce6f"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import plotly.express as px\n",
        "from typing import List, Union, Optional, Dict, Tuple\n",
        "from jaxtyping import Int, Float\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import einops\n",
        "import re\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import webbrowser\n",
        "import gdown\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "from transformer_lens.utils import to_numpy\n",
        "\n",
        "import circuitsvis as cv\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# For Dashiell's groups code\n",
        "from copy import deepcopy\n",
        "from functools import reduce\n",
        "from itertools import product\n",
        "import math\n",
        "import numpy as np\n",
        "from operator import mul\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAIN = __name__ == \"__main__\"\n",
        "\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import os\n",
        "import yaml\n",
        "import s3fs\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from di_automata.config_setup import *\n",
        "from di_automata.constructors import (\n",
        "    construct_model,\n",
        "    create_dataloader_hf,\n",
        ")\n",
        "from di_automata.tasks.data_utils import take_n\n",
        "import plotly.io as pio\n",
        "\n",
        "# AWS\n",
        "load_dotenv()\n",
        "AWS_KEY, AWS_SECRET = os.getenv(\"AWS_KEY\"), os.getenv(\"AWS_SECRET\")\n",
        "s3 = s3fs.S3FileSystem(key=AWS_KEY, secret=AWS_SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from di_automata.interp_utils import (\n",
        "    imshow_attention,\n",
        "    line,\n",
        "    scatter,\n",
        "    imshow,\n",
        "    reorder_list_in_plotly_way,\n",
        "    get_pca,\n",
        "    get_vars,\n",
        "    plot_tensor_heatmap,\n",
        "    get_activations,\n",
        "    LN_hook_names,\n",
        "    get_ln_fit,\n",
        "    cos_sim_with_MLP_weights,\n",
        "    avg_squared_cos_sim,\n",
        "    hook_fn_display_attn_patterns,\n",
        "    hook_fn_patch_qk,\n",
        ")\n",
        "\n",
        "from di_automata.tasks.dashiell_groups import (\n",
        "    DihedralElement,\n",
        "    DihedralIrrep, \n",
        "    ProductDihedralIrrep,\n",
        "    dihedral_conjugacy_classes, \n",
        "    generate_subgroup,\n",
        "    actions_to_labels,\n",
        "    get_all_bits,\n",
        "    dihedral_fourier,\n",
        "    get_fourier_spectrum,\n",
        "    analyse_power,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLSW-IOgijI1"
      },
      "outputs": [],
      "source": [
        "group = DihedralElement.full_group(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amd6_BBPjiuO"
      },
      "outputs": [],
      "source": [
        "translation = {\n",
        "    (0,0):0,\n",
        "    (1,0):1,\n",
        "    (2,0):2,\n",
        "    (3,0):3,\n",
        "    (4,0):4,\n",
        "    (0,1):5,\n",
        "    (1,1):6,\n",
        "    (2,1):7,\n",
        "    (3,1):8,\n",
        "    (4,1):9,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUMf5cdb3d8w"
      },
      "outputs": [],
      "source": [
        "def imshow_attention(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNLip8nbp6vZ"
      },
      "outputs": [],
      "source": [
        "DI_ROOT = Path(\"/content/devinterp-automata-main/\") if IN_COLAB else Path(\"../\")\n",
        "config_file_path = DI_ROOT / f\"scripts/configs/slt_config.yaml\"\n",
        "slt_config = OmegaConf.load(config_file_path)\n",
        "\n",
        "with open(DI_ROOT / f\"scripts/configs/task_config/{slt_config.dataset_type}.yaml\", 'r') as file:\n",
        "    task_config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBUZwD9-Ez-6",
        "outputId": "d7f588e7-8ac1-4143-dcab-d5892dc26c19"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(slt_config, False) # Allow new configuration values to be added\n",
        "# Because we are in Colab and not VSCode, here is where you want to edit your config values\n",
        "slt_config[\"task_config\"] = task_config\n",
        "slt_config[\"lr\"] = 0.0005\n",
        "slt_config[\"num_training_iter\"] = 100000\n",
        "slt_config[\"n_layers\"] = 3\n",
        "\n",
        "# Convert OmegaConf object to MainConfig Pydantic model for dynamic type validation - NECESSARY DO NOT SKIP\n",
        "pydantic_config = PostRunSLTConfig(**slt_config)\n",
        "# Convert back to OmegaConf object for compatibility with existing code\n",
        "slt_config = OmegaConf.create(pydantic_config.model_dump())\n",
        "\n",
        "print(task_config[\"dataset_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEFYiG-EqAuR",
        "outputId": "e8cca4da-ca47-4951-cbc1-49c6ee5341c8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Run path and name for easy referral later\n",
        "run_path = f\"{slt_config.entity_name}/{slt_config.wandb_project_name}\"\n",
        "run_name = slt_config.run_name\n",
        "print(run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lkpLZBEuSRx"
      },
      "outputs": [],
      "source": [
        "# Get run information\n",
        "api = wandb.Api(timeout=3000)\n",
        "run_list = api.runs(\n",
        "    path=run_path,\n",
        "    filters={\n",
        "        \"display_name\": run_name,\n",
        "        \"state\": \"finished\",\n",
        "        },\n",
        "    order=\"created_at\", # Default descending order so backwards in time\n",
        ")\n",
        "assert run_list, f\"Specified run {run_name} does not exist\"\n",
        "run_api = run_list[slt_config.run_idx]\n",
        "try: history = run_api.history()\n",
        "except: history = run_api.history\n",
        "loss_history = history[\"Train Loss\"]\n",
        "accuracy_history = history[\"Train Acc\"]\n",
        "steps = history[\"_step\"]\n",
        "time = run_api.config[\"time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYkD7_cIqEIh"
      },
      "outputs": [],
      "source": [
        "def get_config() -> MainConfig:\n",
        "    \"\"\"\"\n",
        "    Manually get config from run as artifact.\n",
        "    WandB also logs automatically for each run, but it doesn't log enums correctly.\n",
        "    \"\"\"\n",
        "    artifact = api.artifact(f\"{run_path}/config:{run_name}_{time}\")\n",
        "    data_dir = artifact.download()\n",
        "    config_path = Path(data_dir) / \"config.yaml\"\n",
        "    return OmegaConf.load(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgFUGT2lqHoA",
        "outputId": "902ae5a9-1343-401e-9fc2-d6c4e713a052"
      },
      "outputs": [],
      "source": [
        "config = get_config()\n",
        "\n",
        "# Set total number of unique samples seen (n). If this is not done it will break LLC estimator.\n",
        "slt_config.rlct_config.sgld_kwargs.num_samples = slt_config.rlct_config.num_samples = config.rlct_config.sgld_kwargs.num_samples\n",
        "slt_config.nano_gpt_config = config.nano_gpt_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utSRu7dAqPbr"
      },
      "outputs": [],
      "source": [
        "def restore_state_single_cp(cp_idx: int) -> dict:\n",
        "    \"\"\"Restore model state from a single checkpoint.\n",
        "    Used in _load_logits_states() and _calculate_rlct().\n",
        "\n",
        "    Args:\n",
        "        idx_cp: index of checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        model state dictionary.\n",
        "    \"\"\"\n",
        "    idx = cp_idx * config.rlct_config.ed_config.eval_frequency * slt_config.skip_cps\n",
        "    print(f\"Getting checkpoint {idx}\")\n",
        "    print(config.model_save_method)\n",
        "    match config.model_save_method:\n",
        "        case \"wandb\":\n",
        "            artifact = api.artifact(f\"{run_path}/states:idx{idx}_{run_name}_{time}\")\n",
        "            data_dir = artifact.download()\n",
        "            state_path = Path(data_dir) / f\"states_{idx}.torch\"\n",
        "            states = torch.load(state_path)\n",
        "        case \"aws\":\n",
        "            with s3.open(f'{config.aws_bucket}/{run_name}_{time}/states_{idx}.pth', mode='rb') as file:\n",
        "                states = torch.load(file, map_location=device)\n",
        "    return states[\"model\"]\n",
        "\n",
        "\n",
        "def load_logits_single_cp(cp_idx: int) -> None:\n",
        "    \"\"\"Load just a single cp. \n",
        "    This function is designed to be called in multithreading and is called by the above function.\n",
        "    \"\"\"\n",
        "    idx = cp_idx * config.rlct_config.ed_config.eval_frequency * slt_config.skip_cps\n",
        "\n",
        "    try:\n",
        "        match config.model_save_method:\n",
        "            case \"wandb\":\n",
        "                artifact = api.artifact(f\"{run_path}/logits:logits_cp_{idx}_{run_name}_{time}\")\n",
        "                data_dir = artifact.download()\n",
        "                logit_path = Path(data_dir) / f\"logits_cp_{idx}.torch\"\n",
        "                return torch.load(logit_path)\n",
        "            case \"aws\":\n",
        "                with s3.open(f'{config.aws_bucket}/{run_name}_{time}/logits_cp_{idx}.pth', mode='rb') as file:\n",
        "                    return torch.load(file)\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching logits at step {idx}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVaCV7BqiBy",
        "outputId": "2cf8e523-00f7-4d62-fc71-8d12be76d510"
      },
      "outputs": [],
      "source": [
        "current_directory = Path().absolute()\n",
        "logits_file_path = current_directory.parent / f\"di_automata/logits_{run_name}_{time}\"\n",
        "print(logits_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMSwFPbSqnZE"
      },
      "outputs": [],
      "source": [
        "ed_loader = create_dataloader_hf(config, deterministic=True) # Make sure deterministic to see same data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions to display attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv2RNMe656c0"
      },
      "outputs": [],
      "source": [
        "def display_layer_heads(att, batch_idx=0):\n",
        "    \"\"\"For generic inputs, display attention for particular index in batch.\n",
        "    \"\"\"\n",
        "    display(cv.attention.attention_patterns(\n",
        "        tokens=list_of_strings(inputs[batch_idx,...]),\n",
        "        attention=att[batch_idx,...],\n",
        "        attention_head_names=[f\"L0H{i}\" for i in range(4)],\n",
        "    ))\n",
        "    # 0 is toggle action\n",
        "    # 1 is drive action\n",
        "    print(inputs[batch_idx,...])\n",
        "    print(labels[batch_idx,...])\n",
        "\n",
        "\n",
        "def list_of_strings(tensor):\n",
        "    return tensor.numpy().astype(str).tolist()\n",
        "\n",
        "\n",
        "def display_layer_heads_batch(att: torch.Tensor, cache: ActivationCache, toks: list[str]):\n",
        "    \"\"\"TODO: refactor\"\"\"\n",
        "    cv.attention.from_cache(\n",
        "      cache = cache,\n",
        "      tokens = toks,\n",
        "      batch_idx = list(range(10)),\n",
        "      attention_type = \"info-weighted\",\n",
        "      radioitems = True,\n",
        "      return_mode = \"view\",\n",
        "      batch_labels = lambda batch_idx, str_tok_list: format_sequence(str_tok_list, dataset.str_tok_labels[batch_idx]),\n",
        "      mode = \"small\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cp_idxs = [20, 220, 520, 855, 1150, 1500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwIyM5v79Vh8",
        "outputId": "83c3bc52-8672-46b4-aeea-edcbe9c64ed3"
      },
      "outputs": [],
      "source": [
        "# Pre-form\n",
        "cp_idx_0 = 20\n",
        "state_0 = restore_state_single_cp(cp_idx_0)\n",
        "model_0, _ = construct_model(config)\n",
        "model_0.load_state_dict(state_0)\n",
        "\n",
        "# Form 1\n",
        "cp_idx_1 = 220\n",
        "state_1 = restore_state_single_cp(cp_idx_1)\n",
        "model_1, _ = construct_model(config)\n",
        "model_1.load_state_dict(state_1)\n",
        "\n",
        "# Form 2\n",
        "cp_idx_2 = 520\n",
        "state_2 = restore_state_single_cp(cp_idx_2)\n",
        "model_2, _ = construct_model(config)\n",
        "model_2.load_state_dict(state_2)\n",
        "\n",
        "# Form 3\n",
        "cp_idx_3 = 855\n",
        "state_3 = restore_state_single_cp(cp_idx_3)\n",
        "model_3, _ = construct_model(config)\n",
        "model_3.load_state_dict(state_3)\n",
        "\n",
        "# Form 4\n",
        "cp_idx_4 = 1150\n",
        "state_4 = restore_state_single_cp(cp_idx_4)\n",
        "model_4, _ = construct_model(config)\n",
        "model_4.load_state_dict(state_4)\n",
        "\n",
        "# End\n",
        "cp_idx_5 = 1500\n",
        "state_5 = restore_state_single_cp(cp_idx_5)\n",
        "model_5, _ = construct_model(config)\n",
        "model_5.load_state_dict(state_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYi4TAbnUUT2"
      },
      "source": [
        "# Activation distribution PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8LdDYCtsaxc"
      },
      "outputs": [],
      "source": [
        "from plotnine import ggplot, aes, geom_histogram, geom_point, facet_wrap, scale_x_log10, ggtitle\n",
        "import itertools\n",
        "from IPython.display import display\n",
        "import einops\n",
        "import polars as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL2YsyG_kfaq",
        "outputId": "ad5ef1b4-bc1a-46ad-fb6c-35b5c4db447a"
      },
      "outputs": [],
      "source": [
        "data_all_bits = torch.asarray(get_all_bits(16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions_to_labels(torch.tensor([0,1,1,0,1,0,0,0,1,1,1]), translation=translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guM8JYhgv_Q_"
      },
      "outputs": [],
      "source": [
        "all_labels = torch.stack([actions_to_labels(tensor, translation=translation) for tensor in data_all_bits], dim=1)\n",
        "labels = all_labels[15, :]\n",
        "label_df = pl.DataFrame(labels.detach().cpu().numpy(), schema=['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"CUDA not available, but MPS is available.\")\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    print(\"CUDA and MPS not available. Using CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOc-991sls4i",
        "outputId": "323f37e8-c5b5-4623-d7d0-7d90161b1e0b"
      },
      "outputs": [],
      "source": [
        "splits = torch.split(data_all_bits, 512, dim=0)\n",
        "resid_mids = []\n",
        "resid2_mids = []\n",
        "mlp_posts = []\n",
        "mlp2_posts = []\n",
        "\n",
        "model_5.to(device)\n",
        "for batch in tqdm(splits):\n",
        "    with torch.no_grad():\n",
        "        logits_all, cache_all = model_3.run_with_cache(batch.to(device))\n",
        "        resid_mids.append(cache_all[\"resid_mid\", 1].detach().cpu())\n",
        "        resid2_mids.append(cache_all[\"resid_mid\", 2].detach().cpu())\n",
        "        mlp_posts.append(cache_all[\"post\", 1, \"mlp\"].detach().cpu())\n",
        "        mlp2_posts.append(cache_all[\"post\", 2, \"mlp\"].detach().cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Length of all the lists above is 128, since that's how many batches of unique samples we have (aka, 2^16/512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_1iJG3a0Gb2",
        "outputId": "69fea974-6f96-4acc-f843-b363d4f10010"
      },
      "outputs": [],
      "source": [
        "len(resid_mids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimensions of each element of list will be batch size, sequence length, resid dim/mlp dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resid_mids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resid_mids[0].shape\n",
        "mlp_posts[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpxc7Rfp_25s"
      },
      "outputs": [],
      "source": [
        "concat_resids = torch.concatenate(resid_mids, dim=0)\n",
        "concat_resids2 = torch.concatenate(resid2_mids, dim=0)\n",
        "concat_mlp_posts = torch.concatenate(mlp_posts, dim=0)\n",
        "concat_mlp2_posts = torch.concatenate(mlp2_posts, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have concatenated the list elements together we get rid of the batch dimension and end up with all 2^16 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concat_mlp_posts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will focus on position 16 (for no reason at all other than it's the last position!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBmk0N5z0MRD",
        "outputId": "d4ec30fc-d696-40f0-9b16-8349b9251e32"
      },
      "outputs": [],
      "source": [
        "# Get position 16 in sequence (final token)\n",
        "resid_pos16 = concat_resids[:, 15, :]\n",
        "resid2_pos16 = concat_resids2[:, 15, :]\n",
        "mlp_pos16 = concat_mlp_posts[:, 15, :]\n",
        "mlp2_pos16 = concat_mlp2_posts[:, 15, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We got rid of the second dimension, the sequence dimension, by indexing number 15. Nice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resid_pos16.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can also get rid of samples by averaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxzUlQfaNF6K",
        "outputId": "0492047e-0139-4d2f-e656-7601faadd3a2"
      },
      "outputs": [],
      "source": [
        "mlp_pos16.mean(dim=0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for fun part: we want to get the PCA and explained variance of the components of each of these activations at a particular position. Let's start with the MLP post from layer 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqFbaTfh0oWy",
        "outputId": "39150a72-2856-4208-9ac6-e93a15ac28f6"
      },
      "outputs": [],
      "source": [
        "U_mlp, S_mlp, V_mlp = get_pca(mlp2_pos16)\n",
        "get_vars(S_mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is incredibly un-sparse! It basically wants all the available PCA directions for 100% explained variance. Now let's plot the components against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_iMAD1u2u38"
      },
      "outputs": [],
      "source": [
        "mean = mlp2_pos16.mean(dim=0)\n",
        "nonzero = torch.nonzero(mean).squeeze()\n",
        "mlp2_reduced = mlp2_pos16[:, nonzero] @ V_mlp\n",
        "mlp2_posts_df = pl.DataFrame(mlp2_reduced.detach().cpu().numpy(), schema=[str(i) for i in range(mlp2_reduced.shape[1])])\n",
        "mlp2_posts_df = pl.concat([label_df, mlp2_posts_df], how='horizontal')\n",
        "\n",
        "combs = itertools.combinations(range(5), r=2)\n",
        "for c in combs:\n",
        "    plot = ggplot(mlp2_posts_df, aes(x=str(c[0]), y=str(c[1]), color='factor(label)')) + geom_point() + facet_wrap('~label')\n",
        "    display(plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w20xPQsS48_S"
      },
      "outputs": [],
      "source": [
        "#ggplot(df.filter(pl.col('variable').is_in(['0', '1', '2', '3'])), aes(x='value', fill='factor(label)')) + geom_histogram() + facet_wrap('~variable')\n",
        "# mlp2_posts_df.filter(pl.col('0') < -100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U_resid, S_resid, V_resid = get_pca(resid_pos16)\n",
        "get_vars(S_resid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = resid_pos16.mean(dim=0)\n",
        "nonzero = torch.nonzero(mean).squeeze()\n",
        "resid_reduced = resid_pos16[:, nonzero] @ V_resid\n",
        "resid_df = pl.DataFrame(resid_reduced.detach().cpu().numpy(), schema=[str(i) for i in range(resid_reduced.shape[1])])\n",
        "resid_df = pl.concat([label_df, resid_df], how='horizontal')\n",
        "\n",
        "combs = itertools.combinations(range(5), r=2)\n",
        "for c in combs:\n",
        "    plot = ggplot(resid_df, aes(x=str(c[0]), y=str(c[1]), color='factor(label)')) + geom_point() + facet_wrap('~label')\n",
        "    display(plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U_resid2, S_resid2, V_resid2 = get_pca(resid2_pos16)\n",
        "get_vars(S_resid2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = resid2_pos16.mean(dim=0)\n",
        "nonzero = torch.nonzero(mean).squeeze()\n",
        "resid2_reduced = resid2_pos16[:, nonzero] @ V_resid2\n",
        "resid2_df = pl.DataFrame(resid2_reduced.detach().cpu().numpy(), schema=[str(i) for i in range(resid2_reduced.shape[1])])\n",
        "resid2_df = pl.concat([label_df, resid2_df], how='horizontal')\n",
        "\n",
        "combs = itertools.combinations(range(5), r=2)\n",
        "for c in combs:\n",
        "    plot = ggplot(resid2_df, aes(x=str(c[0]), y=str(c[1]), color='factor(label)')) + geom_point() + facet_wrap('~label')\n",
        "    display(plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfZek53UhNs"
      },
      "source": [
        "# Trial inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BCQVv9rUrDl",
        "outputId": "47dd05c5-9465-4155-bbd0-6704250055ee"
      },
      "outputs": [],
      "source": [
        "all1 = torch.ones((25), dtype=torch.int32)\n",
        "all0 = torch.zeros((25), dtype=torch.int32)\n",
        "all1_label = (torch.cumsum(all1, dim=0)) % 5\n",
        "all0_label = torch.tensor([5,0]*12+[5], dtype=torch.int32)\n",
        "print(all1_label)\n",
        "print(all0_label)\n",
        "\n",
        "# All zeros except a single one at one position\n",
        "all_zero_except1 = deepcopy(all0)\n",
        "all_zero_except1[8] = 1\n",
        "all_zero_except1_label = actions_to_labels(all_zero_except1, dtype=\"int\", translation=translation)\n",
        "print(all_zero_except1_label)\n",
        "\n",
        "\n",
        "# All ones except a single zero at one position\n",
        "all_one_except1 = deepcopy(all1)\n",
        "all_one_except1[0] = 0\n",
        "all_one_except1[10] = 0\n",
        "all_one_except1_label = actions_to_labels(all_one_except1, dtype=\"int\", translation=translation)\n",
        "print(all_one_except1_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alternating = torch.tensor([1,0]*12+[1], dtype=torch.int32)\n",
        "alternating_label = actions_to_labels(alternating, dtype=\"int\", translation=translation)\n",
        "alternating_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early in training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQJGcnXxUtK0",
        "outputId": "048ca0ec-e77c-4cee-804b-8d9974f5d3e9"
      },
      "outputs": [],
      "source": [
        "l_all1_0, cache_all1_0 = model_0.run_with_cache(all1)\n",
        "l_all0_0, cache_all0_0 = model_0.run_with_cache(all0)\n",
        "pred_all1_0 = torch.argmax(l_all1_0, dim=-1).squeeze().cpu()\n",
        "pred_all0_0 = torch.argmax(l_all0_0, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_0.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_0)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_0)\n",
        "\n",
        "l_all_zero_except1_0, c_zero_except1_0 = model_0.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_0 , c_all_one_except1_0 = model_0.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_0 = torch.argmax(l_all_zero_except1_0, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_0 = torch.argmax(l_all_one_except1_0, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_0)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_0)\n",
        "\n",
        "l_alt_0, cache_alt_0 = model_0.run_with_cache(alternating)\n",
        "pred_alt_0 = torch.argmax(l_alt_0, dim=-1).squeeze().cpu()\n",
        "print(\"alternating labels\", alternating_label)\n",
        "print(\"predicted alternating\", pred_alt_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_1, cache_all1_1 = model_1.run_with_cache(all1)\n",
        "l_all0_1, cache_all0_1 = model_1.run_with_cache(all0)\n",
        "pred_all1_1 = torch.argmax(l_all1_1, dim=-1).squeeze().cpu()\n",
        "pred_all0_1 = torch.argmax(l_all0_1, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_1.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_1)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_1)\n",
        "\n",
        "l_all_zero_except1_1, c_zero_except1_1 = model_1.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_1 , c_all_one_except1_1 = model_1.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_1 = torch.argmax(l_all_zero_except1_1, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_1 = torch.argmax(l_all_one_except1_1, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_1)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_1)\n",
        "\n",
        "l_alt_1, cache_alt_1 = model_1.run_with_cache(alternating)\n",
        "pred_alt_1 = torch.argmax(l_alt_1, dim=-1).squeeze().cpu()\n",
        "print(\"alternating labels\", alternating_label)\n",
        "print(\"predicted alternating\", pred_alt_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_2, cache_all1_2 = model_2.run_with_cache(all1)\n",
        "l_all0_2, cache_all0_2 = model_2.run_with_cache(all0)\n",
        "pred_all1_2 = torch.argmax(l_all1_2, dim=-1).squeeze().cpu()\n",
        "pred_all0_2 = torch.argmax(l_all0_2, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_2.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_2)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_2)\n",
        "\n",
        "l_all_zero_except1_2, c_zero_except1_2 = model_2.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_2 , c_all_one_except1_2 = model_2.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_2 = torch.argmax(l_all_zero_except1_2, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_2 = torch.argmax(l_all_one_except1_2, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_2)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_2)\n",
        "\n",
        "l_alt_2, cache_alt_2 = model_1.run_with_cache(alternating)\n",
        "pred_alt_2 = torch.argmax(l_alt_2, dim=-1).squeeze().cpu()\n",
        "print(\"alternating labels\", alternating_label)\n",
        "print(\"predicted alternating\", pred_alt_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_3, cache_all1_3 = model_3.run_with_cache(all1)\n",
        "l_all0_3, cache_all0_3 = model_3.run_with_cache(all0)\n",
        "pred_all1_3 = torch.argmax(l_all1_3, dim=-1).squeeze().cpu()\n",
        "pred_all0_3 = torch.argmax(l_all0_3, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_3.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_3)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_3)\n",
        "\n",
        "l_all_zero_except1_3, c_zero_except1_3 = model_3.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_3 , c_all_one_except1_3 = model_3.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_3 = torch.argmax(l_all_zero_except1_3, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_3 = torch.argmax(l_all_one_except1_3, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_3)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_4, cache_all1_4 = model_4.run_with_cache(all1)\n",
        "l_all0_4, cache_all0_4 = model_4.run_with_cache(all0)\n",
        "pred_all1_4 = torch.argmax(l_all1_4, dim=-1).squeeze().cpu()\n",
        "pred_all0_4 = torch.argmax(l_all0_4, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_4.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_4)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_4)\n",
        "\n",
        "l_all_zero_except1_4, c_zero_except1_4 = model_4.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_4 , c_all_one_except1_4 = model_4.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_4 = torch.argmax(l_all_zero_except1_4, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_4 = torch.argmax(l_all_one_except1_4, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_4)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_5, cache_all1_5 = model_5.run_with_cache(all1)\n",
        "l_all0_5, cache_all0_5 = model_5.run_with_cache(all0)\n",
        "pred_all1_5 = torch.argmax(l_all1_5, dim=-1).squeeze().cpu()\n",
        "pred_all0_5 = torch.argmax(l_all0_5, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_5.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_5)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_5)\n",
        "\n",
        "l_all_zero_except1_5, c_all_zero_except1_5 = model_5.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_5 , c_all_one_except1_5 = model_5.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_5 = torch.argmax(l_all_zero_except1_5, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_5 = torch.argmax(l_all_one_except1_5, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_5)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_5)\n",
        "\n",
        "l_alt_5, cache_alt_5 = model_5.run_with_cache(alternating)\n",
        "pred_alt_5 = torch.argmax(l_alt_5, dim=-1).squeeze().cpu()\n",
        "print(\"alternating labels\", alternating_label)\n",
        "print(\"predicted alternating\", pred_alt_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bagm4_EmBila"
      },
      "source": [
        "## First layer attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSoCu74k9X8t"
      },
      "source": [
        "This is a 5-state, 2-layer, 2-head transformer trained to 80% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pass data through\n",
        "for data in take_n(ed_loader, 1):\n",
        "    inputs = data[\"input_ids\"]\n",
        "    labels = data[\"label_ids\"]\n",
        "    break\n",
        "\n",
        "logits_0, cache_0 = model_0.run_with_cache(inputs)\n",
        "logits_1, cache_1 = model_1.run_with_cache(inputs)\n",
        "logits_2, cache_2 = model_2.run_with_cache(inputs)\n",
        "logits_3, cache_3 = model_3.run_with_cache(inputs)\n",
        "logits_4, cache_4 = model_4.run_with_cache(inputs)\n",
        "logits_5, cache_5 = model_5.run_with_cache(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bffQ9LvZU_cP"
      },
      "outputs": [],
      "source": [
        "IDX = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "BmaqKeCt_qDn",
        "outputId": "3a6b4a18-2a2f-4b18-8ff1-f32f103dff1b"
      },
      "outputs": [],
      "source": [
        "att_0_0 = cache_0[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_0_0, batch_idx=IDX+15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "GN1vmzk9AP5i",
        "outputId": "e8017672-8c1a-42b6-fc73-2618800586d1"
      },
      "outputs": [],
      "source": [
        "att_1_0 = cache_1[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_1_0, batch_idx=IDX)\n",
        "display_layer_heads(att_1_0, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "lCryJp9cnE4J",
        "outputId": "07544a80-da97-499f-e137-b4952016823b"
      },
      "outputs": [],
      "source": [
        "att_2_0 = cache_2[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_2_0, batch_idx=IDX)\n",
        "display_layer_heads(att_2_0, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "8YnwTFREcQ4X",
        "outputId": "040168bb-ae5f-43f9-eb12-291dfce02218"
      },
      "outputs": [],
      "source": [
        "att_3_0 = cache_3[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_3_0, batch_idx=IDX)\n",
        "display_layer_heads(att_3_0, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_4_0 = cache_4[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_4_0, batch_idx=IDX)\n",
        "display_layer_heads(att_4_0, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_0 = cache_5[\"pattern\", 0, \"attn\"]\n",
        "display_layer_heads(att_5_0, batch_idx=IDX)\n",
        "display_layer_heads(att_5_0, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvetCS-0BQ0P"
      },
      "source": [
        "## Second layer attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "XMYx1F3_-jI5",
        "outputId": "eee67101-0b51-4e62-f3c5-6231ea333519"
      },
      "outputs": [],
      "source": [
        "att_0_1 = cache_0[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_0_1, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "o83dGV5__ynD",
        "outputId": "042fcbd5-2f5e-42a2-ca18-8c9da8024f9b"
      },
      "outputs": [],
      "source": [
        "att_1_1 = cache_1[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_1_1, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "BnQ9IJ2ZmeMf",
        "outputId": "87c7d466-34b9-4a2c-ab9d-30d90db3c950"
      },
      "outputs": [],
      "source": [
        "att_2_1 = cache_2[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_2_1, batch_idx=IDX)\n",
        "display_layer_heads(att_2_1, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "Lw_fLQqWcf4z",
        "outputId": "b0333c8a-28c6-456f-a194-19ed47499994"
      },
      "outputs": [],
      "source": [
        "att_3_1 = cache_3[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_3_1, batch_idx=IDX)\n",
        "display_layer_heads(att_3_1, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_4_1 = cache_4[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_4_1, batch_idx=IDX)\n",
        "display_layer_heads(att_4_1, batch_idx=IDX+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_1 = cache_5[\"pattern\", 1, \"attn\"]\n",
        "display_layer_heads(att_5_1, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Third layer attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_0_3 = cache_0[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_0_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_1_3 = cache_1[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_1_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_2_3 = cache_2[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_2_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_3_3 = cache_3[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_3_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_4_3 = cache_4[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_4_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_3 = cache_5[\"pattern\", 2, \"attn\"]\n",
        "display_layer_heads(att_5_3, batch_idx=IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trial input attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All 1s\n",
        "Let's do all 1s as the input for the final model, and observe all 3 layers' attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EOGHk9AoRsR-",
        "outputId": "15d3dc08-fe81-4322-e013-5f8f30dee382"
      },
      "outputs": [],
      "source": [
        "att_5_0_all1 = cache_all1_5[\"pattern\", 0, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_0_all1[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_1_all1 = cache_all1_5[\"pattern\", 1, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_1_all1[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_2_all1 = cache_all1_5[\"pattern\", 2, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_2_all1[0,head,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All 0s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_0_all0 = cache_all0_5[\"pattern\", 0, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_0_all0[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_1_all0 = cache_all0_5[\"pattern\", 1, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_1_all0[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_2_all0 = cache_all0_5[\"pattern\", 2, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_2_all0[0,head,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_0_alt = cache_alt_5[\"pattern\", 0, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_0_alt[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_1_alt = cache_alt_5[\"pattern\", 1, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_1_alt[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_2_alt = cache_alt_5[\"pattern\", 2, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_2_alt[0,head,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Form 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_1_0_all1 = cache_all1_1[\"pattern\", 0, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_1_0_all1[0,head,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All 1s except 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_0_all_one_except1 = c_all_one_except1_5[\"pattern\", 0, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_0_all_one_except1[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_1_all_one_except1 = c_all_one_except1_5[\"pattern\", 1, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_1_all_one_except1[0,head,...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "att_5_2_all_one_except1 = c_all_one_except1_5[\"pattern\", 2, \"attn\"]\n",
        "for head in range(4):\n",
        "    imshow_attention(att_5_2_all_one_except1[0,head,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activation patching/ablation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from jaxtyping import Float\n",
        "import transformer_lens.utils as utils\n",
        "\n",
        "def ablate_hook(\n",
        "    value: Float[torch.Tensor, \"input_vocab d_model\"],\n",
        "    hook: HookPoint,\n",
        ") -> Float[torch.Tensor, \"input_vocab d_model\"]:\n",
        "    value[:, :] = -0.\n",
        "    return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w_pos_hook_name = utils.get_act_name(\"resid_post\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_1.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = model_5.run_with_hooks(\n",
        "    all1,\n",
        "    fwd_hooks=[(\n",
        "        \"hook_pos_embed\",\n",
        "        ablate_hook,\n",
        "    )]\n",
        ")\n",
        "print(torch.argmax(logits, dim=-1))\n",
        "model_5.reset_hooks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_5.W_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_5.W_pos @ model_5.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_0.W_pos @ model_0.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_1.W_pos @ model_1.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_2.W_pos @ model_2.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_3.W_pos @ model_3.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imshow_attention(model_4.W_pos @ model_4.W_pos.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = model_0.run_with_hooks(\n",
        "    alternating,\n",
        "    fwd_hooks=[(\n",
        "    \"hook_pos_embed\",\n",
        "    ablate_hook,\n",
        "    )]\n",
        ")\n",
        "print(torch.argmax(logits, dim=-1))\n",
        "model_0.reset_hooks()\n",
        "\n",
        "print(torch.argmax(l_all0_0, dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_5.QK[0,...][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=-1)\n",
        "for layer_num in range(3):\n",
        "    for head_num in range(4):\n",
        "        imshow_attention(model_5.W_pos @ model_5.QK.AB[layer_num, ...][head_num].T)\n",
        "        imshow_attention(softmax(model_5.QK.AB[layer_num, ...][head_num]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_5.W_V.shape\n",
        "# for layer in range(3):\n",
        "#     for head in range(4):\n",
        "#         imshow_attention(model_5.W_V[layer, ...][head].T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0H1jjC5td_"
      },
      "source": [
        "# OV circuit analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTopEpN_5wbd",
        "outputId": "8bb648c8-7d9c-4870-9f2a-d35abdd8ef02"
      },
      "outputs": [],
      "source": [
        "print(cache_1[\"scale\"].shape)\n",
        "# Layernorm scale, [batch, pos, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBDMmooKRXAq",
        "outputId": "dddf87d2-e1f7-4a95-b292-785a9bcb3ba8"
      },
      "outputs": [],
      "source": [
        "# [nlayers nheads dmodel dhead] x [nlayers nheads dmodel dhead].T\n",
        "W_OV = model_0.W_V @ model_0.W_O # [nlayers nheads dmodel dmodel]\n",
        "W_E = model_0.W_E # [vocab_in dhead]\n",
        "W_U = model_0.W_U # [vocab_out dhead]\n",
        "print(W_E.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5yghQPRSkaF"
      },
      "outputs": [],
      "source": [
        "scale_final = cache_1[\"scale\"][:, :, 0].mean()\n",
        "scale_0 = cache_1[\"scale\", 0, \"ln1\"].mean()\n",
        "scale_1 = cache_1[\"scale\", 1, \"ln1\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsqqnbnMByqP",
        "outputId": "37b8765d-b3a5-4983-fa8e-e66ab3434a6c"
      },
      "outputs": [],
      "source": [
        "print(W_OV[1].shape)\n",
        "print(W_OV[0].shape)\n",
        "print(W_OV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-54mrBvTB4G",
        "outputId": "8bd05be4-5f59-42da-8cac-75b805c3885d"
      },
      "outputs": [],
      "source": [
        "# ! Get direct path\n",
        "W_E_OV_direct = (W_E / scale_final) @ W_U\n",
        "print(f\"Direct {W_E_OV_direct.shape}\") # [vocab_out vocab_out]\n",
        "\n",
        "# ! Get full OV matrix for path through just layer 0\n",
        "W_E_OV_0 = (W_E / scale_0) @ W_OV[0]\n",
        "W_OV_0_full = (W_E_OV_0 / scale_final) @ W_U # [n_head vocab_in vocab_out]\n",
        "print(f\"Layer 0 {W_OV_0_full.shape}\")\n",
        "\n",
        "# ! Get full OV matrix for path through just layer 1\n",
        "W_E_OV_1 = (W_E / scale_1) @ W_OV[1]\n",
        "W_OV_1_full = (W_E_OV_1 / scale_final) @ W_U # [n_head vocab_in vocab_out]\n",
        "print(f\"Layer 1 {W_OV_1_full.shape}\")\n",
        "\n",
        "# ! Get full OV matrix for path through heads in layer 0 and 1\n",
        "W_E_OV_01 = einops.einsum(\n",
        "    (W_E_OV_0 / scale_1), W_OV[1],\n",
        "    \"head0 vocab_in d_model_in, head1 d_model_in d_model_out -> head0 head1 vocab_in d_model_out\",\n",
        ")\n",
        "W_OV_01_full = (W_E_OV_01 / scale_final) @ W_U # [head0 head1 vocab_in vocab_out]\n",
        "print(f\"Layers 0, 1 {W_OV_01_full.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-adcLnLVLGk",
        "outputId": "fb8e8b25-e711-45a9-824d-dfa323ba6ce4"
      },
      "outputs": [],
      "source": [
        "print(W_E_OV_direct[None, None].shape)\n",
        "print(W_OV_0_full[:, None].shape)\n",
        "print(W_OV_1_full[None].shape)\n",
        "print(W_OV_01_full.shape)\n",
        "\n",
        "cat_1 = torch.cat([W_E_OV_direct[None, None], W_OV_0_full[:, None]]) # [head0 1 vocab_in vocab_out]\n",
        "cat_2 = torch.cat([W_OV_1_full[None], W_OV_01_full])  # [head0 head1 vocab_in vocab_out]\n",
        "print(cat_1.shape, cat_2.shape)\n",
        "\n",
        "W_OV_full_all = torch.cat([\n",
        "    cat_1,\n",
        "    cat_2,\n",
        "], dim=1) # [head0 head1 vocab_in vocab_out]\n",
        "print(W_OV_full_all.shape)\n",
        "print(W_OV_full_all.transpose(0, 1).flatten(0, 1).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h96abzNLYSup",
        "outputId": "13111f8c-b978-47ed-db46-a26b2206dad5"
      },
      "outputs": [],
      "source": [
        "tokens = [str(i) for i in range(10)]\n",
        "components_0 = [\"W<sub>E</sub>\"] + [f\"0.{i}\" for i in range(4)]\n",
        "components_1 = [\"W<sub>U</sub>\"] + [f\"1.{i}\" for i in range(4)]\n",
        "\n",
        "# Using dict.fromkeys() prevents repeats\n",
        "facet_labels = [\"  \".join(list(dict.fromkeys([\"W<sub>E</sub>\", c0, c1, \"W<sub>U</sub>\"]))) for c1 in components_1 for c0 in components_0]\n",
        "imshow(\n",
        "    W_OV_full_all.transpose(0, 1).flatten(0, 1), # .softmax(dim=-1),\n",
        "    facet_col = 0,\n",
        "    facet_col_wrap = 5,\n",
        "    facet_labels = facet_labels,\n",
        "    title = f\"Full virtual OV circuits\",\n",
        "    x = tokens,\n",
        "    y = tokens,\n",
        "    labels = {\"x\": \"Source\", \"y\": \"Dest\"},\n",
        "    height = 1200,\n",
        "    width = 1200,\n",
        "    # text = text,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_5.to('cpu')\n",
        "OV_circuit_all_heads = model_5.OV\n",
        "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in TfLens model\", zmax=1.0, zmin=-1.0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
