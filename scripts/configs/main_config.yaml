defaults:
  - task_config: adder

dataset_type: adder
model_type: NANO_GPT
parameterisation: MUP

calc_llc_train: true
calc_ed_train: false
calc_llc_checkpoint: false
num_training_iter: 20
eval_frequency: 10 # If not specified then use length of dataset (dataloader length)
loss_threshold: 0.01
num_eval_batches: 20
model_save_path: "trained_models"
save_local: false # Probably change if running on cloud
run_name: null
is_wandb_enabled: false
num_epochs: null

optimizer_config:
  optimizer_type: ADAMW
  default_lr: 3e-3
  final_lr: 3e-4
  global_lr: 1.0
  optimizer_kwargs: {}
  per_param_lr: {}
  weight_decay: 0
  clip_grad: 1.0 # Change to inf if training is being weird
  cosine_lr_schedule: true
  
dataloader_config:
  train_bs: 32
  test_bs: 32
  num_workers: 4
  train_fraction: 1
  shuffle_train: true

nano_gpt_config:
  block_size: 100
  vocab_size: 4 # Will be updated automatically based on dataset
  output_vocab_size: 2 # Will be updated automatically based on dataset
  n_layers: 3
  n_heads: 8
  embed_dim: 512
  dropout: 0
  is_causal: true
  bias: true

rlct_config:
  sampling_method: SGLD
  rlct_loss_type: ce
  sigma: 0.25
  num_chains: 5
  num_draws: 100
  num_burnin_steps: 0 
  num_steps_bw_draws: 1
  batch_size: 1024
  cores: 1
  seed: null

  use_diagnostics: true
  online: false
  verbose: true
  return_weights: true
  use_distill_loss: true
  save_results: true

  sgld_kwargs:
    lr: 5e-7
    noise_level: 1.0
    weight_decay: 3e-7
    elasticity: 10.
    temperature: adaptive
    num_samples: 500 # Default could also be length of dataset, but here have iterable
  sgnht_kwargs:
    lr: 5e-7
    diffusion_factor: 0.01
    bounding_box_size: 0.5
    num_samples: 500 # Default could also be length of dataset, but here have iterable
  ed_config:
    batches_per_checkpoint: 200

  rlct_model_save_dir: null
  rlct_data_dir: null

wandb_config:
  log_to_wandb: true
  save_model_as_artifact: true
  wandb_project_name: devinterp-automata
  sweep: false
  entity_name: wu-cindyx

initialisation:
  default_init_scale: 1.0
  global_init_scale: 1.0
  init_scales_per_param: {}
  init_distribution: NORMAL