defaults:
  - task_config: adder

model_type: NANO_GPT
parameterisation: MUP

llc_train: true
ed_train: false
use_ema: true

num_training_iter: 14000
eval_frequency: 50 # If not specified then use length of dataset (dataloader length)
ema_decay: 0.9
num_eval_batches: 20
early_stop_patience: 20
early_stop_acc_threshold: 99.9

rlct_config:
  sampling_method: SGLD_MA
  rlct_loss_type: ce # When not using distill kl loss
  num_samples: null # Set by validator
  num_chains: 5
  num_draws: 5000
  num_burnin_steps: 0 
  num_steps_bw_draws: 1
  cores: 1
  seed: null

  use_diagnostics: true
  online: false
  verbose: true
  use_distill_loss: true

  ed_config:
    batches_per_checkpoint: 300
    eval_frequency: 10

  sgld_kwargs:
    lr: 5e-8
    noise_level: 1.0
    weight_decay: 5e-8
    elasticity: 10
    temperature: adaptive
    num_samples: 10000 # True value set at run-time.
    bounding_box_size: 0.5
    mh_frequency: 5

  rlct_model_save_dir: null
  rlct_data_dir: null

run_name: null
is_wandb_enabled: false
num_epochs: null

optimizer_config:
  optimizer_type: ADAMW
  default_lr: 1e-4
  final_lr: 1e-5
  global_lr: 1.0
  optimizer_kwargs: {}
  per_param_lr: {}
  weight_decay: 0
  clip_grad: 10 # Change to inf if training is being weird
  cosine_lr_schedule: true
  
dataloader_config:
  train_bs: 64
  test_bs: 64
  num_workers: 4
  train_fraction: 1
  shuffle_train: true

nano_gpt_config:
  block_size: 100
  vocab_size: 4 # Will be updated automatically based on dataset
  output_vocab_size: 2 # Will be updated automatically based on dataset
  n_layers: 10
  n_heads: 8
  embed_dim: 512
  dropout: 0.0
  is_causal: true
  bias: true

wandb_config:
  log_to_wandb: true
  save_model_as_artifact: true
  wandb_project_name: devinterp-automata-alpha
  sweep: false
  entity_name: wu-cindyx

initialisation:
  default_init_scale: 1.0
  global_init_scale: 1.0
  init_scales_per_param: {}
  init_distribution: NORMAL