defaults:
  - task_config: adder

dataset_type: adder
model_type: NANO_GPT
parameterisation: MUP

num_training_iter: 100000
eval_frequency: 500 # If not specified then use length of dataset (dataloader length)
loss_threshold: 0.01
num_eval_batches: 20
model_save_path: "trained_models"
save_local: true # Probably change if running on cloud
run_name: null
is_wandb_enabled: false
num_epochs: null

optimizer_config:
  optimizer_type: ADAMW
  default_lr: 3e-3
  global_lr: 1.0
  optimizer_kwargs: {}
  per_param_lr: {}
  weight_decay: 0
  clip_grad: 1.0 # Change to inf if training is being weird
  cosine_lr_schedule: false
  
dataloader_config:
  train_bs: 32
  test_bs: 32
  num_workers: 4
  train_fraction: 1
  shuffle_train: true

nano_gpt_config:
  block_size: 100
  vocab_size: 4 # Will be updated automatically based on dataset
  output_vocab_size: 2 # Will be updated automatically based on dataset
  n_layers: 3
  n_heads: 8
  embed_dim: 512
  dropout: 0
  is_causal: true
  bias: true

rlct_config:
  sampling_method: SGLD
  sigma: 0.25
  sgld_kwargs:
    lr: 5e-7
    noise_level: 1.0
    weight_decay: 3e-7
    elasticity: 1.0
    temperature: "adaptive"
    num_samples: 10000 # Default could also be length of dataset, but here have iterable
  sgnht_kwargs:
    lr: 5e-7
    diffusion_factor: 0.01
    bounding_box_size: 0.5
    num_samples: 10000 # Default could also be length of dataset, but here have iterable
  num_chains: 10
  num_draws: 100
  num_burnin_steps: 0 
  num_steps_bw_draws: 1
  batch_size: 1024
  cores: 1
  seed:
    # Default: None. Can be an int or a list of ints.
    # Example: 1234 or [1234, 5678]
  pbar: true  # Progress bar
  verbose: true
  return_weights: true
  use_distill_loss: true
  save_results: true

wandb_config:
  log_to_wandb: true
  save_model_as_artifact: true
  wandb_project_name: "devinterp-automata"
  sweep: false

initialisation:
  default_init_scale: 1.0
  global_init_scale: 1.0
  init_scales_per_param: {}
  init_distribution: NORMAL