{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rageo\\Anaconda3\\envs\\devinterp\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:151: UserWarning: Field \"model_type\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rageo\\Anaconda3\\envs\\devinterp\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:151: UserWarning: Field \"model_save_method\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import s3fs\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "\n",
    "from di_automata.config_setup import *\n",
    "from di_automata.constructors import (\n",
    "    construct_model, \n",
    "    create_dataloader_hf,\n",
    ")\n",
    "from di_automata.io import read_tensors_from_file, append_tensor_to_file\n",
    "\n",
    "# AWS\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dihedral\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_file_path = \"configs/slt_config.yaml\"\n",
    "slt_config = OmegaConf.load(config_file_path)\n",
    "\n",
    "with open(f\"configs/task_config/{slt_config.dataset_type}.yaml\", 'r') as file:\n",
    "    task_config = yaml.safe_load(file)\n",
    "    \n",
    "OmegaConf.set_struct(slt_config, False) # Allow new configuration values to be added\n",
    "slt_config[\"task_config\"] = task_config\n",
    "# Convert OmegaConf object to MainConfig Pydantic model for dynamic type validation - NECESSARY DO NOT SKIP\n",
    "pydantic_config = PostRunSLTConfig(**slt_config)\n",
    "# Convert back to OmegaConf object for compatibility with existing code\n",
    "slt_config = OmegaConf.create(pydantic_config.model_dump())\n",
    "\n",
    "print(task_config[\"dataset_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run path and name for easy referral later\n",
    "run_path = f\"{slt_config.entity_name}/{slt_config.wandb_project_name}-alpha\"\n",
    "run_name = slt_config.run_name\n",
    "\n",
    "# Get run information\n",
    "api = wandb.Api()\n",
    "run_list = api.runs(\n",
    "    path=run_path, \n",
    "    filters={\n",
    "        \"display_name\": run_name,\n",
    "        \"state\": \"finished\",\n",
    "        },\n",
    "    order=\"created_at\", # Default descending order so backwards in time\n",
    ")\n",
    "run_api = run_list[slt_config.run_idx]\n",
    "try: history = run_api.history()\n",
    "except: history = run_api.history\n",
    "loss_history = history[\"Train Loss\"]\n",
    "accuracy_history = history[\"Train Acc\"]\n",
    "steps = history[\"_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of artifacts\n",
    "artifacts = run_api.logged_artifacts()\n",
    "artifact_list = []\n",
    "for artifact in run_api.logged_artifacts():\n",
    "    artifact_list.append(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() -> MainConfig:\n",
    "    \"\"\"\"\n",
    "    Manually get config from run as artifact. \n",
    "    WandB also logs automatically for each run, but it doesn't log enums correctly.\n",
    "    \"\"\"\n",
    "    artifact = artifact = api.artifact(f\"{run_path}/states:dihedral_config_state\")\n",
    "    # artifact = api.artifact(f\"{run_path}/config:{run_name}\")\n",
    "    # artifact = api.artifact(f\"{run_path}/states:idx{idx}_{run_name}\")\n",
    "    data_dir = artifact.download()\n",
    "    config_path = Path(data_dir) / \"config.yaml\"\n",
    "    return OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact states:dihedral_config_state, 180.95MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    }
   ],
   "source": [
    "# config: MainConfig = OmegaConf.create(run_api.config)\n",
    "config = get_config()\n",
    "config[\"model_save_method\"] = \"wandb\" # Duct tape\n",
    "\n",
    "# Set total number of unique samples seen (n). If this is not done it will break LLC estimator.\n",
    "slt_config.rlct_config.sgld_kwargs.num_samples = slt_config.rlct_config.num_samples = config.rlct_config.sgld_kwargs.num_samples\n",
    "slt_config.nano_gpt_config = config.nano_gpt_config\n",
    "\n",
    "model, param_inf_properties = construct_model(config)\n",
    "\n",
    "# Optional: currently don't use as local logits take up a lot of storage\n",
    "logits_path = \"logits.bin\" # Binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_state(checkpoint_idx: int) -> dict:\n",
    "    \"\"\"Restore one model state from a checkpoint. Called only once for any given checkpoint.\n",
    "    Intention of this function is to be used to load individual points of interest after plotting essential dynamics osculating circles.\n",
    "    \n",
    "    Params:\n",
    "        checkpoint_idx: Index in steps.\n",
    "        \n",
    "    Returns:\n",
    "        model state dictionary.\n",
    "    \"\"\"\n",
    "    match config.model_save_method:\n",
    "        case \"wandb\":\n",
    "            artifact = artifact_list[checkpoint_idx]\n",
    "            data_dir = artifact.download()\n",
    "            model_state_path = Path(data_dir) / \"states.torch\"\n",
    "            states = torch.load(model_state_path)\n",
    "        case \"aws\":\n",
    "            with s3.open(f\"{config.aws_bucket}/{config.run_name}_{config.time}/{checkpoint_idx}\") as f:\n",
    "                states = torch.load(f)\n",
    "    return states[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "cusp_idx = 621\n",
    "cp_idx = cusp_idx // config.rlct_config.ed_config.eval_frequency\n",
    "print(cp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact states:v1956, 180.95MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = restore_state(cp_idx)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (token_embedding): Embedding(2, 512)\n",
      "  (pos_embedding): Embedding(26, 512)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (h): Sequential(\n",
      "    (block0): TransformerBlock(\n",
      "      (ln_1): LayerNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (in_projection): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): Lambda()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (block1): TransformerBlock(\n",
      "      (ln_1): LayerNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (in_projection): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): Lambda()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (block2): TransformerBlock(\n",
      "      (ln_1): LayerNorm()\n",
      "      (attn): SelfAttention(\n",
      "        (in_projection): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (gelu): Lambda()\n",
      "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (unembedding): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=512, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Linear(in_features=512, out_features=1536, bias=True)>\n"
     ]
    }
   ],
   "source": [
    "# Pain I could rewrite the attention layer so it returns things, or just move to TFLens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
