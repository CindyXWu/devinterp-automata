{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IMIMIXuoqUT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import os, sys\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    if not os.path.exists(\"chapter1_transformers\"):\n",
        "        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/ARENA_2.0/zip/refs/heads/main\n",
        "        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n",
        "        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n",
        "        os.rmdir(\"ARENA_2.0-main\")\n",
        "\n",
        "         # Install packages\n",
        "        %pip install einops\n",
        "        %pip install jaxtyping\n",
        "        %pip install transformer_lens\n",
        "        %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "        %pip install s3fs\n",
        "        %pip install omegaconf\n",
        "        %pip install git+https://github.com/CindyXWu/devinterp-automata.git\n",
        "        %pip install torch-ema\n",
        "\n",
        "        !curl -o /content/main.zip https://codeload.github.com/CindyXWu/devinterp-automata/zip/refs/heads/main\n",
        "        !unzip -o /content/main.zip -d /content/\n",
        "\n",
        "        sys.path.append(\"/content/devinterp-automata/\")\n",
        "        os.remove(\"/content/main.zip\")\n",
        "\n",
        "        os.chdir(\"chapter1_transformers/exercises\")\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
        "\n",
        "    CHAPTER = r\"chapter1_transformers\"\n",
        "    CHAPTER_DIR = r\"./\" if CHAPTER in os.listdir() else os.getcwd().split(CHAPTER)[0]\n",
        "    EXERCISES_DIR = CHAPTER_DIR + f\"{CHAPTER}/exercises\"\n",
        "    sys.path.append(EXERCISES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87aqhevcowv4",
        "outputId": "0e2bd4b2-192c-48c6-939d-8ed81aa8ce6f"
      },
      "outputs": [],
      "source": [
        "from dotenv.main import load_dotenv\n",
        "import plotly.express as px\n",
        "from typing import List, Union, Optional, Dict, Tuple\n",
        "from jaxtyping import Int, Float\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import einops\n",
        "import re\n",
        "import functools\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "from transformer_lens.utils import to_numpy\n",
        "\n",
        "# Plotting\n",
        "import circuitsvis as cv\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Dashiell's groups code\n",
        "from copy import deepcopy\n",
        "from functools import reduce\n",
        "from itertools import product\n",
        "import math\n",
        "import numpy as np\n",
        "from operator import mul\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAIN = __name__ == \"__main__\"\n",
        "\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import os\n",
        "import yaml\n",
        "import s3fs\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from di_automata.config_setup import *\n",
        "from di_automata.constructors import (\n",
        "    construct_model,\n",
        "    create_dataloader_hf,\n",
        ")\n",
        "from di_automata.tasks.data_utils import take_n\n",
        "import plotly.io as pio\n",
        "\n",
        "# AWS\n",
        "load_dotenv()\n",
        "AWS_KEY, AWS_SECRET = os.getenv(\"AWS_KEY\"), os.getenv(\"AWS_SECRET\")\n",
        "s3 = s3fs.S3FileSystem(key=AWS_KEY, secret=AWS_SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from di_automata.interp_utils import (\n",
        "    imshow_attention,\n",
        "    line,\n",
        "    scatter,\n",
        "    imshow,\n",
        "    reorder_list_in_plotly_way,\n",
        "    get_pca,\n",
        "    get_vars,\n",
        "    plot_tensor_heatmap,\n",
        "    get_activations,\n",
        "    LN_hook_names,\n",
        "    get_ln_fit,\n",
        "    cos_sim_with_MLP_weights,\n",
        "    avg_squared_cos_sim,\n",
        "    hook_fn_display_attn_patterns,\n",
        "    hook_fn_patch_qk,\n",
        ")\n",
        "\n",
        "from di_automata.tasks.dashiell_groups import (\n",
        "    DihedralElement,\n",
        "    DihedralIrrep, \n",
        "    ProductDihedralIrrep,\n",
        "    dihedral_conjugacy_classes, \n",
        "    generate_subgroup,\n",
        "    actions_to_labels,\n",
        "    get_all_bits,\n",
        "    dihedral_fourier,\n",
        "    get_fourier_spectrum,\n",
        "    analyse_power,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dihedral representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amd6_BBPjiuO"
      },
      "outputs": [],
      "source": [
        "group = DihedralElement.full_group(5)\n",
        "translation = {\n",
        "    (0,0):0,\n",
        "    (1,0):1,\n",
        "    (2,0):2,\n",
        "    (3,0):3,\n",
        "    (4,0):4,\n",
        "    (0,1):5,\n",
        "    (1,1):6,\n",
        "    (2,1):7,\n",
        "    (3,1):8,\n",
        "    (4,1):9,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dihedral_conjugacy_classes(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DihedralIrrep(5, (0, 0)).dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dihedral_fn = torch.tensor([-1, 1, -1, 1, -1, 1, -1, 1, -1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sequence is the 'anti' of the parity irreducible representation of the dihedral group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(dihedral_fn * DihedralIrrep(5, (0, 1)).tensors()).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dihedral_fourier(fn_values, n):\n",
        "    irreps = {conj: DihedralIrrep(n, conj).tensors() for conj in dihedral_conjugacy_classes(n)}\n",
        "    return {conj: (fn_values * irrep).sum(dim=0) for conj, irrep in irreps.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep_list = [\n",
        "    DihedralIrrep(5, (0, 0)).tensors(), \n",
        "    DihedralIrrep(5, (0, 1)).tensors(),\n",
        "    DihedralIrrep(5, (1, 0)).tensors(),\n",
        "    DihedralIrrep(5, (2, 0)).tensors(),\n",
        "]\n",
        "for rep in rep_list: print(rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dihedral_fourier(torch.randn((10, 1)), 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DihedralIrrep(5, (0, 1)).tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUMf5cdb3d8w"
      },
      "outputs": [],
      "source": [
        "def imshow_attention(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNLip8nbp6vZ"
      },
      "outputs": [],
      "source": [
        "DI_ROOT = Path(\"/content/devinterp-automata-main/\") if IN_COLAB else Path(\"../\")\n",
        "config_file_path = DI_ROOT / f\"scripts/configs/slt_config.yaml\"\n",
        "slt_config = OmegaConf.load(config_file_path)\n",
        "\n",
        "with open(DI_ROOT / f\"scripts/configs/task_config/{slt_config.dataset_type}.yaml\", 'r') as file:\n",
        "    task_config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBUZwD9-Ez-6",
        "outputId": "d7f588e7-8ac1-4143-dcab-d5892dc26c19"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(slt_config, False) # Allow new configuration values to be added\n",
        "# Because we are in Colab and not VSCode, here is where you want to edit your config values\n",
        "slt_config[\"task_config\"] = task_config\n",
        "slt_config[\"lr\"] = 0.0005\n",
        "slt_config[\"num_training_iter\"] = 100000\n",
        "slt_config[\"n_layers\"] = 3\n",
        "\n",
        "# Convert OmegaConf object to MainConfig Pydantic model for dynamic type validation - NECESSARY DO NOT SKIP\n",
        "pydantic_config = PostRunSLTConfig(**slt_config)\n",
        "# Convert back to OmegaConf object for compatibility with existing code\n",
        "slt_config = OmegaConf.create(pydantic_config.model_dump())\n",
        "\n",
        "print(task_config[\"dataset_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEFYiG-EqAuR",
        "outputId": "e8cca4da-ca47-4951-cbc1-49c6ee5341c8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Run path and name for easy referral later\n",
        "run_path = f\"{slt_config.entity_name}/{slt_config.wandb_project_name}\"\n",
        "run_name = slt_config.run_name\n",
        "print(run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lkpLZBEuSRx"
      },
      "outputs": [],
      "source": [
        "# Get run information\n",
        "api = wandb.Api(timeout=3000)\n",
        "run_list = api.runs(\n",
        "    path=run_path,\n",
        "    filters={\n",
        "        \"display_name\": run_name,\n",
        "        \"state\": \"finished\",\n",
        "        },\n",
        "    order=\"created_at\", # Default descending order so backwards in time\n",
        ")\n",
        "assert run_list, f\"Specified run {run_name} does not exist\"\n",
        "run_api = run_list[slt_config.run_idx]\n",
        "try: history = run_api.history()\n",
        "except: history = run_api.history\n",
        "loss_history = history[\"Train Loss\"]\n",
        "accuracy_history = history[\"Train Acc\"]\n",
        "steps = history[\"_step\"]\n",
        "time = run_api.config[\"time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYkD7_cIqEIh"
      },
      "outputs": [],
      "source": [
        "def get_config() -> MainConfig:\n",
        "    \"\"\"\"\n",
        "    Manually get config from run as artifact.\n",
        "    WandB also logs automatically for each run, but it doesn't log enums correctly.\n",
        "    \"\"\"\n",
        "    artifact = api.artifact(f\"{run_path}/config:{run_name}_{time}\")\n",
        "    data_dir = artifact.download()\n",
        "    config_path = Path(data_dir) / \"config.yaml\"\n",
        "    return OmegaConf.load(config_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgFUGT2lqHoA",
        "outputId": "902ae5a9-1343-401e-9fc2-d6c4e713a052"
      },
      "outputs": [],
      "source": [
        "config = get_config()\n",
        "\n",
        "# Set total number of unique samples seen (n). If this is not done it will break LLC estimator.\n",
        "slt_config.rlct_config.sgld_kwargs.num_samples = slt_config.rlct_config.num_samples = config.rlct_config.sgld_kwargs.num_samples\n",
        "slt_config.nano_gpt_config = config.nano_gpt_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restoring logits and checkpointed states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utSRu7dAqPbr"
      },
      "outputs": [],
      "source": [
        "def restore_state_single_cp(cp_idx: int) -> dict:\n",
        "    \"\"\"Restore model state from a single checkpoint.\n",
        "    Used in _load_logits_states() and _calculate_rlct().\n",
        "\n",
        "    Args:\n",
        "        idx_cp: index of checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        model state dictionary.\n",
        "    \"\"\"\n",
        "    idx = cp_idx * config.rlct_config.ed_config.eval_frequency * slt_config.skip_cps\n",
        "    print(f\"Getting checkpoint {idx}\")\n",
        "    print(config.model_save_method)\n",
        "    match config.model_save_method:\n",
        "        case \"wandb\":\n",
        "            artifact = api.artifact(f\"{run_path}/states:idx{idx}_{run_name}_{time}\")\n",
        "            data_dir = artifact.download()\n",
        "            state_path = Path(data_dir) / f\"states_{idx}.torch\"\n",
        "            states = torch.load(state_path)\n",
        "        case \"aws\":\n",
        "            with s3.open(f'{config.aws_bucket}/{run_name}_{time}/states_{idx}.pth', mode='rb') as file:\n",
        "                states = torch.load(file, map_location=device)\n",
        "    return states[\"model\"]\n",
        "\n",
        "\n",
        "def load_logits_single_cp(cp_idx: int) -> None:\n",
        "    \"\"\"Load just a single cp. \n",
        "    This function is designed to be called in multithreading and is called by the above function.\n",
        "    \"\"\"\n",
        "    idx = cp_idx * config.rlct_config.ed_config.eval_frequency * slt_config.skip_cps\n",
        "\n",
        "    try:\n",
        "        match config.model_save_method:\n",
        "            case \"wandb\":\n",
        "                artifact = api.artifact(f\"{run_path}/logits:logits_cp_{idx}_{run_name}_{time}\")\n",
        "                data_dir = artifact.download()\n",
        "                logit_path = Path(data_dir) / f\"logits_cp_{idx}.torch\"\n",
        "                return torch.load(logit_path)\n",
        "            case \"aws\":\n",
        "                with s3.open(f'{config.aws_bucket}/{run_name}_{time}/logits_cp_{idx}.pth', mode='rb') as file:\n",
        "                    return torch.load(file)\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching logits at step {idx}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVaCV7BqiBy",
        "outputId": "2cf8e523-00f7-4d62-fc71-8d12be76d510"
      },
      "outputs": [],
      "source": [
        "current_directory = Path().absolute()\n",
        "logits_file_path = current_directory.parent / f\"di_automata/logits_{run_name}_{time}\"\n",
        "print(logits_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ed_loader` below has seed=42 set, so can recreate the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMSwFPbSqnZE"
      },
      "outputs": [],
      "source": [
        "ed_loader = create_dataloader_hf(config, deterministic=True) # Make sure deterministic to see same data\n",
        "eval_loader = create_dataloader_hf(config, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions to display attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv2RNMe656c0"
      },
      "outputs": [],
      "source": [
        "def display_layer_heads(att, batch_idx=0):\n",
        "    \"\"\"For generic inputs, display attention for particular index in batch.\n",
        "    \"\"\"\n",
        "    display(cv.attention.attention_patterns(\n",
        "        tokens=list_of_strings(inputs[batch_idx,...]),\n",
        "        attention=att[batch_idx,...],\n",
        "        attention_head_names=[f\"L0H{i}\" for i in range(4)],\n",
        "    ))\n",
        "    # 0 is toggle action\n",
        "    # 1 is drive action\n",
        "    print(inputs[batch_idx,...])\n",
        "    print(labels[batch_idx,...])\n",
        "\n",
        "\n",
        "def list_of_strings(tensor):\n",
        "    return tensor.numpy().astype(str).tolist()\n",
        "\n",
        "\n",
        "def display_layer_heads_batch(att: torch.Tensor, cache: ActivationCache, toks: list[str]):\n",
        "    \"\"\"TODO: refactor\"\"\"\n",
        "    cv.attention.from_cache(\n",
        "      cache = cache,\n",
        "      tokens = toks,\n",
        "      batch_idx = list(range(10)),\n",
        "      attention_type = \"info-weighted\",\n",
        "      radioitems = True,\n",
        "      return_mode = \"view\",\n",
        "      batch_labels = lambda batch_idx, str_tok_list: format_sequence(str_tok_list, dataset.str_tok_labels[batch_idx]),\n",
        "      mode = \"small\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cp_idxs = [20, 220, 520, 855, 1150, 1500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwIyM5v79Vh8",
        "outputId": "83c3bc52-8672-46b4-aeea-edcbe9c64ed3"
      },
      "outputs": [],
      "source": [
        "# Pre-form\n",
        "cp_idx_0 = 20\n",
        "state_0 = restore_state_single_cp(cp_idx_0)\n",
        "model_0, _ = construct_model(config)\n",
        "model_0.load_state_dict(state_0)\n",
        "\n",
        "# Form 1\n",
        "cp_idx_1 = 220\n",
        "state_1 = restore_state_single_cp(cp_idx_1)\n",
        "model_1, _ = construct_model(config)\n",
        "model_1.load_state_dict(state_1)\n",
        "\n",
        "# Form 2\n",
        "cp_idx_2 = 520\n",
        "state_2 = restore_state_single_cp(cp_idx_2)\n",
        "model_2, _ = construct_model(config)\n",
        "model_2.load_state_dict(state_2)\n",
        "\n",
        "# Form 3\n",
        "cp_idx_3 = 855\n",
        "state_3 = restore_state_single_cp(cp_idx_3)\n",
        "model_3, _ = construct_model(config)\n",
        "model_3.load_state_dict(state_3)\n",
        "\n",
        "# Form 4\n",
        "cp_idx_4 = 1150\n",
        "state_4 = restore_state_single_cp(cp_idx_4)\n",
        "model_4, _ = construct_model(config)\n",
        "model_4.load_state_dict(state_4)\n",
        "\n",
        "# End\n",
        "cp_idx_5 = 1500\n",
        "state_5 = restore_state_single_cp(cp_idx_5)\n",
        "model_5, _ = construct_model(config)\n",
        "model_5.load_state_dict(state_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sImbNx19xbt"
      },
      "outputs": [],
      "source": [
        "# Pass data through\n",
        "for data in take_n(ed_loader, 1):\n",
        "    inputs = data[\"input_ids\"]\n",
        "    labels = data[\"label_ids\"]\n",
        "    break\n",
        "\n",
        "logits_0, cache_0 = model_0.run_with_cache(inputs)\n",
        "logits_1, cache_1 = model_1.run_with_cache(inputs)\n",
        "logits_2, cache_2 = model_2.run_with_cache(inputs)\n",
        "logits_3, cache_3 = model_3.run_with_cache(inputs)\n",
        "logits_4, cache_4 = model_4.run_with_cache(inputs)\n",
        "logits_5, cache_5 = model_5.run_with_cache(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get logits for forms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "from einops import rearrange\n",
        "\n",
        "base_path = Path.cwd().parent\n",
        "pca_path = base_path / Path(f\"di_automata/ed_data/{run_name}_{time}/pca\")\n",
        "with open(pca_path, 'rb') as f:\n",
        "    pca = torch.load(f)\n",
        "pca_vectors = pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_form(form_num: int):\n",
        "    form_path = base_path / Path(f\"di_automata/ed_data/{run_name}_{time}/forms/form_{cp_idxs[form_num]}\")\n",
        "    with open(form_path, 'rb') as f:\n",
        "        form = pickle.load(f)\n",
        "    form_logits = pca_vectors[0,:] * form[0] + pca_vectors[1,:] * form[1] + pca_vectors[2,:] * form[2]\n",
        "    form_logits = rearrange(form_logits, '(n b c s) -> n b s c', n=config.rlct_config.ed_config.batches_per_checkpoint, b=config.dataloader_config.train_bs, c=config.tflens_config.d_vocab_out, s=config.task_config.length)\n",
        "    return form_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get forms.\n",
        "Logits have shape 150 (number of evaluation batches) x 64 (batch size) x 25 (sequence length) x 10 (class number) = (2.4e6,)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "form_1_logits = torch.from_numpy(load_form(1))\n",
        "form_2_logits = torch.from_numpy(load_form(2))\n",
        "form_3_logits = torch.from_numpy(load_form(3))\n",
        "form_4_logits = torch.from_numpy(load_form(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fourier transform of form logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_labels(form_logits):\n",
        "    \"\"\"Get labels for ed dataset used to create form logits.\"\"\"\n",
        "    ed_loader = create_dataloader_hf(config, deterministic=True)\n",
        "    labels = [] # Stores all batches of labels\n",
        "    for data in take_n(ed_loader, form_logits.shape[0]):\n",
        "        labels.append(data[\"label_ids\"])\n",
        "    return torch.stack(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ed_labels = get_labels(form_1_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(form_1_logits, ed_labels, form=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(form_2_logits, ed_labels, form=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(form_3_logits, ed_labels, form=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(form_4_logits, ed_labels, form=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These unfortunately aren't that informative - it may be that I need more PCA directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fourier transform of closest checkpoint logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pass data through\n",
        "for data in take_n(eval_loader, 1):\n",
        "    eval_inputs = data[\"input_ids\"]\n",
        "    eval_labels = data[\"label_ids\"]\n",
        "    break\n",
        "\n",
        "logits_eval_1, cache_eval_1 = model_1.run_with_cache(eval_inputs)\n",
        "logits_eval_2, cache_eval_2 = model_2.run_with_cache(eval_inputs)\n",
        "logits_eval_3, cache_eval_3 = model_3.run_with_cache(eval_inputs)\n",
        "logits_eval_4, cache_eval_4 = model_4.run_with_cache(eval_inputs)\n",
        "logits_eval_5, cache_eval_5 = model_5.run_with_cache(eval_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits_eval_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Where labela are class 0\n",
        "mask = eval_labels == 0\n",
        "mean_vals_1 = logits_eval_1.mean(dim=(0,1))\n",
        "mean_vals_1 = mean_vals_1[torch.tensor([0, 5, 1, 6, 2, 7, 3, 8, 4, 9])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split up Fourier transform by even or odd label. In reality to fully carve this up we should have a grid where rows correspond to labels and columns to predicions. This is also done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "values = torch.tensor([0., 1., 2., 3., 4.], dtype=torch.float32)\n",
        "even_mask = torch.zeros(eval_labels.size(), dtype=torch.bool)\n",
        "for value in values:\n",
        "    even_mask = even_mask | (eval_labels == value)\n",
        "\n",
        "transform_all = dihedral_fourier(mean_vals_1.to('cpu').unsqueeze(1), 5)\n",
        "transform_all\n",
        "\n",
        "even_mean_1 = logits_eval_1[even_mask].mean(dim=0)\n",
        "even_transform = dihedral_fourier(even_mean_1.to('cpu').unsqueeze(1), 5)\n",
        "even_transform\n",
        "# even_mean_1 -= mean_vals_1\n",
        "\n",
        "odd_mean_1 = logits_eval_1[~even_mask].mean(dim=(0))\n",
        "odd_transform = dihedral_fourier(odd_mean_1.to('cpu').unsqueeze(1), 5)\n",
        "odd_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms_1, means_1 = get_fourier_spectrum(logits_eval_1, eval_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(transforms_1, means_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms_2, means_2 = get_fourier_spectrum(logits_eval_2, eval_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(transforms_2, means_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(logits_eval_3, eval_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(logits_eval_4, eval_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyse_power(*get_fourier_spectrum(logits_eval_5, eval_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These logit results don't quite make sense - the zero power on the parity (0,1) mode for all checkpoints doesn't check out with its ability to learn the parity feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfZek53UhNs"
      },
      "source": [
        "# Trial inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BCQVv9rUrDl",
        "outputId": "47dd05c5-9465-4155-bbd0-6704250055ee"
      },
      "outputs": [],
      "source": [
        "all1 = torch.ones((25), dtype=torch.int32)\n",
        "all0 = torch.zeros((25), dtype=torch.int32)\n",
        "all1_label = (torch.cumsum(all1, dim=0)) % 5\n",
        "all0_label = torch.tensor([5,0]*12+[5], dtype=torch.int32)\n",
        "print(all1_label)\n",
        "print(all0_label)\n",
        "\n",
        "# All zeros except a single one at one position\n",
        "all_zero_except1 = deepcopy(all0)\n",
        "all_zero_except1[8] = 1\n",
        "all_zero_except1_label = actions_to_labels(all_zero_except1, translation, dtype=\"int\")\n",
        "print(all_zero_except1_label)\n",
        "\n",
        "\n",
        "# All ones except a single zero at one position\n",
        "all_one_except1 = deepcopy(all1)\n",
        "all_one_except1[8] = 0\n",
        "all_one_except1_label = actions_to_labels(all_one_except1, translation, dtype=\"int\",)\n",
        "print(all_one_except1_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Early in training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_0, cache_all1_0 = model_0.run_with_cache(all1)\n",
        "l_all0_0, cache_all0_0 = model_0.run_with_cache(all0)\n",
        "pred_all1_0 = torch.argmax(l_all1_0, dim=-1).squeeze().cpu()\n",
        "pred_all0_0 = torch.argmax(l_all0_0, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_0.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_0)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_0)\n",
        "\n",
        "l_all_zero_except1_0, c_zero_except1_0 = model_0.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_0 , c_all_one_except1_0 = model_0.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_0 = torch.argmax(l_all_zero_except1_0, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_0 = torch.argmax(l_all_one_except1_0, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_0)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_0)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_0, all0_label, pred_all0_0, all_zero_except1_label, pred_all_zero_except1_0, all_one_except1_label, pred_all_one_except1_0])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_1, cache_all1_1 = model_1.run_with_cache(all1)\n",
        "l_all0_1, cache_all0_1 = model_1.run_with_cache(all0)\n",
        "pred_all1_1 = torch.argmax(l_all1_1, dim=-1).squeeze().cpu()\n",
        "pred_all0_1 = torch.argmax(l_all0_1, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_1.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_1)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_1)\n",
        "\n",
        "l_all_zero_except1_1, c_zero_except1_1 = model_1.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_1 , c_all_one_except1_1 = model_1.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_1 = torch.argmax(l_all_zero_except1_1, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_1 = torch.argmax(l_all_one_except1_1, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_1)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_1)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_1, all0_label, pred_all0_1, all_zero_except1_label, pred_all_zero_except1_1, all_one_except1_label, pred_all_one_except1_1])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_2, cache_all1_2 = model_2.run_with_cache(all1)\n",
        "l_all0_2, cache_all0_2 = model_2.run_with_cache(all0)\n",
        "pred_all1_2 = torch.argmax(l_all1_2, dim=-1).squeeze().cpu()\n",
        "pred_all0_2 = torch.argmax(l_all0_2, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_2.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all ones\", pred_all1_2)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all zeros\", pred_all0_2)\n",
        "\n",
        "l_all_zero_except1_2, c_zero_except1_2 = model_2.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_2 , c_all_one_except1_2 = model_2.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_2 = torch.argmax(l_all_zero_except1_2, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_2 = torch.argmax(l_all_one_except1_2, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_2)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_2)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_2, all0_label, pred_all0_2, all_zero_except1_label, pred_all_zero_except1_2, all_one_except1_label, pred_all_one_except1_2])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQJGcnXxUtK0",
        "outputId": "048ca0ec-e77c-4cee-804b-8d9974f5d3e9"
      },
      "outputs": [],
      "source": [
        "l_all1_3, cache_all1_3 = model_3.run_with_cache(all1)\n",
        "l_all0_3, cache_all0_3 = model_3.run_with_cache(all0)\n",
        "pred_all1_3 = torch.argmax(l_all1_3, dim=-1).squeeze().cpu()\n",
        "pred_all0_3 = torch.argmax(l_all0_3, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_3.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_3)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_3)\n",
        "\n",
        "l_all_zero_except1_3, c_zero_except1_3 = model_3.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_3 , c_all_one_except1_3 = model_3.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_3 = torch.argmax(l_all_zero_except1_3, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_3 = torch.argmax(l_all_one_except1_3, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_3)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_3)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_3, all0_label, pred_all0_3, all_zero_except1_label, pred_all_zero_except1_3, all_one_except1_label, pred_all_one_except1_3])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_4, cache_all1_4 = model_4.run_with_cache(all1)\n",
        "l_all0_4, cache_all0_4 = model_4.run_with_cache(all0)\n",
        "pred_all1_4 = torch.argmax(l_all1_4, dim=-1).squeeze().cpu()\n",
        "pred_all0_4 = torch.argmax(l_all0_4, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_4.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_4)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_4)\n",
        "\n",
        "l_all_zero_except1_4, c_zero_except1_4 = model_4.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_4 , c_all_one_except1_4 = model_4.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_4 = torch.argmax(l_all_zero_except1_4, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_4 = torch.argmax(l_all_one_except1_4, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_4)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_4)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_4, all0_label, pred_all0_4, all_zero_except1_label, pred_all_zero_except1_4, all_one_except1_label, pred_all_one_except1_4])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_all1_5, cache_all1_5 = model_5.run_with_cache(all1)\n",
        "l_all0_5, cache_all0_5 = model_5.run_with_cache(all0)\n",
        "pred_all1_5 = torch.argmax(l_all1_5, dim=-1).squeeze().cpu()\n",
        "pred_all0_5 = torch.argmax(l_all0_5, dim=-1).squeeze().cpu()\n",
        "print(pred_all0_5.shape)\n",
        "print(\"all one labels\", all1_label)\n",
        "print(\"predicted all 1s\", pred_all1_5)\n",
        "print(\"all zero labels\", all0_label)\n",
        "print(\"predicted all 0s\", pred_all0_5)\n",
        "\n",
        "l_all_zero_except1_5, c_zero_except1_5 = model_5.run_with_cache(all_zero_except1)\n",
        "l_all_one_except1_5 , c_all_one_except1_5 = model_5.run_with_cache(all_one_except1)\n",
        "pred_all_zero_except1_5 = torch.argmax(l_all_zero_except1_5, dim=-1).squeeze().cpu()\n",
        "pred_all_one_except1_5 = torch.argmax(l_all_one_except1_5, dim=-1).squeeze().cpu()\n",
        "print(all_zero_except1_label)\n",
        "print(\"predicted all 0s except 1\", pred_all_zero_except1_5)\n",
        "print(all_one_except1_label)\n",
        "print(\"predicted all 1s except 1\", pred_all_one_except1_5)\n",
        "\n",
        "# Convert tensors to numpy arrays for easier handling with seaborn\n",
        "tensor_matrix = torch.stack([all1_label, pred_all1_5, all0_label, pred_all0_5, all_zero_except1_label, pred_all_zero_except1_5, all_one_except1_label, pred_all_one_except1_5])\n",
        "plot_tensor_heatmap(tensor_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
