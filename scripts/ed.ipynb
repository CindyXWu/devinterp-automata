{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from typing import TypeVar\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import s3fs\n",
    "from queue import Queue\n",
    "\n",
    "from einops import rearrange\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "\n",
    "from di_automata.devinterp.rlct_utils import (\n",
    "    plot_pca_plotly,\n",
    "    plot_explained_var,\n",
    ")\n",
    "from di_automata.config_setup import *\n",
    "from di_automata.constructors import (\n",
    "    construct_model, \n",
    "    create_dataloader_hf,\n",
    "    construct_rlct_criterion,\n",
    ")\n",
    "from di_automata.tasks.data_utils import take_n\n",
    "from di_automata.io import read_tensors_from_file, append_tensor_to_file\n",
    "from di_automata.devinterp.ed_utils import EssentialDynamicsPlotter\n",
    "Sweep = TypeVar(\"Sweep\")\n",
    "\n",
    "# AWS\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General setup: read SLT config \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_path = \"configs/slt_config.yaml\"\n",
    "slt_config = OmegaConf.load(config_file_path)\n",
    "\n",
    "with open(f\"configs/task_config/{slt_config.dataset_type}.yaml\", 'r') as file:\n",
    "    task_config = yaml.safe_load(file)\n",
    "    \n",
    "OmegaConf.set_struct(slt_config, False) # Allow new configuration values to be added\n",
    "slt_config[\"task_config\"] = task_config\n",
    "# Convert OmegaConf object to MainConfig Pydantic model for dynamic type validation - NECESSARY DO NOT SKIP\n",
    "pydantic_config = PostRunSLTConfig(**slt_config)\n",
    "# Convert back to OmegaConf object for compatibility with existing code\n",
    "slt_config = OmegaConf.create(pydantic_config.model_dump())\n",
    "\n",
    "print(task_config[\"dataset_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup run\n",
    "# Run path and name for easy referral later\n",
    "run_path = f\"{slt_config.entity_name}/{slt_config.wandb_project_name}-alpha\"\n",
    "run_name = slt_config.run_name\n",
    "\n",
    "# Get run information\n",
    "api = wandb.Api()\n",
    "run_list = api.runs(\n",
    "    path=run_path, \n",
    "    filters={\n",
    "        \"display_name\": run_name,\n",
    "        \"state\": \"finished\",\n",
    "        },\n",
    "    order=\"created_at\", # Default descending order so backwards in time\n",
    ")\n",
    "run_api = run_list[slt_config.run_idx]\n",
    "try: history = run_api.history()\n",
    "except: history = run_api.history\n",
    "loss_history = history[\"Train Loss\"]\n",
    "accuracy_history = history[\"Train Acc\"]\n",
    "steps = history[\"_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get queue of artifacts from old run\n",
    "artifacts = run_api.logged_artifacts()\n",
    "artifact_queue = Queue()\n",
    "for artifact in run_api.logged_artifacts():\n",
    "    artifact_queue.put(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() -> MainConfig:\n",
    "    \"\"\"\"\n",
    "    Manually get config from run as artifact. \n",
    "    WandB also logs automatically for each run, but it doesn't log enums correctly.\n",
    "    \"\"\"\n",
    "    artifact = artifact = api.artifact(f\"{run_path}/states:dihedral_config_state\")\n",
    "    # artifact = api.artifact(f\"{run_path}/config:{run_name}\")\n",
    "    # artifact = api.artifact(f\"{run_path}/states:idx{idx}_{run_name}\")\n",
    "    data_dir = artifact.download()\n",
    "    config_path = Path(data_dir) / \"config.yaml\"\n",
    "    return OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and set up configs\n",
    "# config: MainConfig = OmegaConf.create(run_api.config)\n",
    "config = get_config()\n",
    "config[\"model_save_method\"] = \"wandb\" # Duct tape\n",
    "\n",
    "# Set total number of unique samples seen (n). If this is not done it will break LLC estimator.\n",
    "slt_config.rlct_config.sgld_kwargs.num_samples = slt_config.rlct_config.num_samples = config.rlct_config.sgld_kwargs.num_samples\n",
    "slt_config.nano_gpt_config = config.nano_gpt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(config) -> None:\n",
    "    \"\"\"Call at initialisation to set loggers to WandB and/or AWS.\n",
    "    Run naming convention is preprend 'post' to distinguish from training runs.\n",
    "    \"\"\"\n",
    "    config[\"slt_config\"] = slt_config # For saving to WandB\n",
    "    # Add previous run id to tie runs together\n",
    "    config[\"prev_run_path\"] = f\"{run_path}/{run_api.id}\"\n",
    "    logger_params = {\n",
    "        \"name\": f\"post_{config.run_name}\",\n",
    "        \"project\": config.wandb_config.wandb_project_name,\n",
    "        # \"settings\": wandb.Settings(start_method=\"thread\"),\n",
    "        \"config\": OmegaConf.to_container(config, resolve=True, throw_on_missing=True),\n",
    "        \"mode\": \"disabled\" if not config.is_wandb_enabled else \"online\",\n",
    "    }\n",
    "    run = wandb.init(**logger_params, entity=config.wandb_config.entity_name)\n",
    "    \n",
    "    # Location on remote GPU of WandB cache to delete periodically\n",
    "    wandb_cache_dirs = [Path.home() / \".cache/wandb/artifacts/obj\", Path.home() / \"root/.cache/wandb/artifacts/obj\"]\n",
    "    \n",
    "    return run, wandb_cache_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_ed_logits(ed_logits: list[torch.Tensor]) -> None:\n",
    "    \"\"\"Truncate run to clean out overtraining at end for cleaner ED plots.\n",
    "    Determines the cutoff index for early stopping based on log loss.\n",
    "    \"\"\"\n",
    "    # Manually specify cutoff index\n",
    "    if slt_config.truncate_its is not None:\n",
    "        total_its = len(loss_history) * config.eval_frequency\n",
    "        ed_logit_cutoff_idx = len(ed_logits) * slt_config.truncate_its // total_its\n",
    "        ed_logits = ed_logits[:ed_logit_cutoff_idx]\n",
    "        return\n",
    "    \n",
    "    # Automatically calculate cutoff index using early-stop patience\n",
    "    log_loss_values = np.log(loss_history.to_numpy())\n",
    "    smoothed_log_loss = np.convolve(log_loss_values, np.ones(slt_config.early_stop_smoothing_window)/slt_config.early_stop_smoothing_window, mode='valid')\n",
    "\n",
    "    increases = 0\n",
    "    for i in range(1, len(smoothed_log_loss)):\n",
    "        if smoothed_log_loss[i] > smoothed_log_loss[i-1]:\n",
    "            increases += 1\n",
    "            if increases >= config.early_stop_patience:\n",
    "                # Index where the increase trend starts\n",
    "                cutoff_idx = (i - slt_config.early_stop_patience + 1) * config.eval_frequency # Cutoff idx in loss step\n",
    "                ed_logit_cutoff_idx = cutoff_idx * config.rlct_config.ed_config.eval_frequency // config.eval_frequency\n",
    "                ed_logits = ed_logits[:ed_logit_cutoff_idx]\n",
    "        else:\n",
    "            increases = 0\n",
    "    \n",
    "    return ed_logits\n",
    "\n",
    "\n",
    "def ed_calculation(self, ed_logits: Optional[list[torch.Tensor]]) -> np.ndarray:\n",
    "    \"\"\"PCA and plot part of ED.\n",
    "    \n",
    "    Diplay top 3 components against each other and show fraction variance explained.\n",
    "    \"\"\"\n",
    "    if os.path.exists(self.logits_path):\n",
    "        ed_logits = read_tensors_from_file(self.logits_path, self.config)\n",
    "        # Delete logits as these can take up to 30GB of storage\n",
    "        os.remove(\"logits.bin\")\n",
    "    \n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(ed_logits.cpu().numpy())\n",
    "    \n",
    "    # Projected coordinates for plotting purposes\n",
    "    pca_projected_samples = np.empty((len(ed_logits), 3))\n",
    "    for i, row in enumerate(ed_logits):\n",
    "        logits_epoch = rearrange(row, 'n -> 1 n').cpu().numpy()\n",
    "        projected_vector = pca.transform(logits_epoch)[0]\n",
    "        pca_projected_samples[i] = projected_vector\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    plot_pca_plotly(pca_projected_samples[:,0], pca_projected_samples[:,1], pca_projected_samples[:,2], self.config)\n",
    "    plot_explained_var(explained_variance)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"ED_PCA_truncated\": wandb.Image(\"PCA.png\"),\n",
    "        \"explained_var_truncated\": wandb.Image(\"pca_explained_var.png\"),\n",
    "    })\n",
    "    \n",
    "    pca_samples_file_path = f\"pca_projected_samples_{self.config.run_name}.pkl\"\n",
    "    with open(pca_samples_file_path, 'wb') as f:\n",
    "        pickle.dump(pca_projected_samples, f)\n",
    "\n",
    "    ## Try to avoid saving logits to WandB because they can be REAL CHONK (50-100GB PER RUN)\n",
    "    # self._save_logits() \n",
    "    \n",
    "    return pca_projected_samples\n",
    "\n",
    "\n",
    "def get_ed_logits_from_checkpoints() -> list[torch.Tensor]:\n",
    "    \"\"\"For each checkpoint, do forward pass to obtain logits and save.\n",
    "    Note checkpoint weights were saved with EMA if config.use_ema is True.\n",
    "    \"\"\"\n",
    "    ed_logits = []\n",
    "    \n",
    "    for checkpoint_idx in range(1, config.num_training_iter // config.rlct_config.ed_config.eval_frequency):\n",
    "        # idx = checkpoint_idx * config.rlct_config.ed_config.eval_frequency\n",
    "        state_dict = restore_states()\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        logits_epoch = []\n",
    "        with torch.no_grad():\n",
    "            for data in take_n(ed_loader, config.rlct_config.ed_config.batches_per_checkpoint):\n",
    "                inputs = data[\"input_ids\"].to(device)\n",
    "                logits = model(inputs)\n",
    "                # Flatten over batch, class and sequence dimension\n",
    "                logits_epoch.append(rearrange(logits, 'b c s -> (b c s)'))\n",
    "        \n",
    "        # Concat all per-batch logits over batch dimension to form one super-batch\n",
    "        logits_epoch = torch.cat(logits_epoch)\n",
    "        \n",
    "        # # Append to binary file\n",
    "        # append_tensor_to_file(logits_epoch, logits_path)\n",
    "        \n",
    "        ed_logits.append(logits_epoch)\n",
    "    \n",
    "    return ed_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_states() -> dict:\n",
    "    \"\"\"Called for every checkpoint in the run in strict order.\n",
    "    This produces a queue of artifact states, which are popped and read in turn to \n",
    "    generate the essential dynamics PCA matrix.\n",
    "    \n",
    "    TODO: edit reading from queue for AWS.\n",
    "    \n",
    "    Params:\n",
    "        idx: Index in steps.\n",
    "        \n",
    "    Returns:\n",
    "        model state dictionary.\n",
    "    \"\"\"\n",
    "    match config.model_save_method:\n",
    "        case \"wandb\":\n",
    "            artifact = artifact_queue.get()\n",
    "            data_dir = artifact.download()\n",
    "            model_state_path = Path(data_dir) / \"states.torch\"\n",
    "            states = torch.load(model_state_path)\n",
    "        # case \"aws\":\n",
    "        #     with s3.open(f\"{config.aws_bucket}/{config.run_name}_{config.time}/{idx}\") as f:\n",
    "        #         states = torch.load(f)\n",
    "    return states[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main executable code for essential dynamics osculating circle calculation.\"\"\"\n",
    "# config: MainConfig = OmegaConf.create(run_api.config)\n",
    "config = get_config()\n",
    "config[\"model_save_method\"] = \"wandb\" # Duct tape\n",
    "slt_config.nano_gpt_config = config.nano_gpt_config\n",
    "\n",
    "# Log old config and SLT config to new run for post-analysis information\n",
    "# START WANDB LOGGING\n",
    "run, wandb_cache_dirs = set_logger(config)\n",
    "\n",
    "ed_loader = create_dataloader_hf(config, deterministic=True) # Make sure deterministic to see same data\n",
    "\n",
    "model, param_inf_properties = construct_model(config)\n",
    "\n",
    "current_directory = Path().absolute()\n",
    "logits_file_path = current_directory.parent / f\"di_automata/logits_{run_name}_{time}\"\n",
    "print(logits_file_path)\n",
    "\n",
    "if os.path.exists(logits_file_path):\n",
    "    print(f\"Loading existing logits from {logits_file_path}\")\n",
    "    ed_logits = torch.load(logits_file_path)\n",
    "    print(\"Done loading existing logits\")\n",
    "else:\n",
    "    ed_logits: list[torch.Tensor] = get_ed_logits_from_checkpoints()\n",
    "    \n",
    "ed_logits: list[torch.Tensor] = truncate_ed_logits(ed_logits)\n",
    "ed_projected_samples = ed_calculation(ed_logits)\n",
    "\n",
    "# Create and call instance of essential dynamics osculating circle plotter\n",
    "ed_plotter = EssentialDynamicsPlotter(ed_projected_samples, steps, slt_config.ed_plot_config, run_name)\n",
    "wandb.log({\"ed_osculating\": wandb.Image(\"ed_osculating_circles.png\")})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "upload_cache_dir = Path.home() / \"root/.local/share/wandb/artifacts/staging\" \n",
    "if upload_cache_dir.is_dir():\n",
    "    shutil.rmtree(upload_cache_dir)\n",
    "    \n",
    "time.sleep(60)\n",
    "shutil.rmtree(\"wandb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
